{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r2A21W9EHF_p"
   },
   "outputs": [],
   "source": [
    "# import gensim\n",
    "import collections\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import progressbar\n",
    "\n",
    "import altair as alt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3V8kKxAKJ9b"
   },
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UH9j-D4BeU1q"
   },
   "outputs": [],
   "source": [
    "LOG_DIR = \"universal_embeddings\"\n",
    "path_for_metadata = os.path.join(LOG_DIR,'metadata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "muHD6w9CHa1Q",
    "outputId": "b268b330-b683-4fb9-a85e-8a0ef9817ac8"
   },
   "outputs": [],
   "source": [
    "DATADIR = os.getenv(\"DATADIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOZZ-trpszRZ"
   },
   "outputs": [],
   "source": [
    "labelled = pd.read_csv(os.path.join(DATADIR, 'labelled.csv.gz'), compression='gzip', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "I8qjmfrftyFY",
    "outputId": "a218a89e-007f-4528-9c5b-c91bb017399f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_path</th>\n",
       "      <th>content_id</th>\n",
       "      <th>description</th>\n",
       "      <th>document_type</th>\n",
       "      <th>first_published_at</th>\n",
       "      <th>locale</th>\n",
       "      <th>primary_publishing_organisation</th>\n",
       "      <th>publishing_app</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>taxon_id</th>\n",
       "      <th>taxon_base_path</th>\n",
       "      <th>taxon_name</th>\n",
       "      <th>level1taxon</th>\n",
       "      <th>level2taxon</th>\n",
       "      <th>level3taxon</th>\n",
       "      <th>level4taxon</th>\n",
       "      <th>level5taxon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>285092</th>\n",
       "      <td>/guidance/tonnage-tax-for-shipping-companies</td>\n",
       "      <td>601f0b66-7631-11e4-a3cb-005056011aef</td>\n",
       "      <td>understand how you can pay tonnage tax if you ...</td>\n",
       "      <td>detailed_guide</td>\n",
       "      <td>2011-12-13T00:00:00.000+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>HM Revenue &amp; Customs</td>\n",
       "      <td>whitehall</td>\n",
       "      <td>find out how to pay tonnage tax if you're a sh...</td>\n",
       "      <td>overview tonnage tax is a form of corporation ...</td>\n",
       "      <td>find out how to pay tonnage tax if you're a sh...</td>\n",
       "      <td>0f0ba370-1824-4326-85a3-c4038a0a3783</td>\n",
       "      <td>/money/business-tax-international-tax-shipping</td>\n",
       "      <td>Shipping</td>\n",
       "      <td>Money</td>\n",
       "      <td>Business tax</td>\n",
       "      <td>International tax</td>\n",
       "      <td>Shipping</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285093</th>\n",
       "      <td>/government/collections/infectious-diseases-in...</td>\n",
       "      <td>bf520108-e372-4a6e-ad8d-9e7ed7bd065d</td>\n",
       "      <td>information for healthcare professionals provi...</td>\n",
       "      <td>document_collection</td>\n",
       "      <td>2016-03-02T15:04:00.000+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>Public Health England</td>\n",
       "      <td>whitehall</td>\n",
       "      <td>infectious diseases in pregnancy screening: cl...</td>\n",
       "      <td>these documents explain the procedures for pro...</td>\n",
       "      <td>infectious diseases in pregnancy screening: cl...</td>\n",
       "      <td>1fe05c70-0bd9-4725-9ed4-3abc0f5f7961</td>\n",
       "      <td>/health-and-social-care/population-screening-p...</td>\n",
       "      <td>Commission and provide services</td>\n",
       "      <td>Health and social care</td>\n",
       "      <td>Population screening programmes</td>\n",
       "      <td>NHS infectious diseases in pregnancy screening...</td>\n",
       "      <td>Commission and provide services</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285094</th>\n",
       "      <td>/guidance/rhoi-gwybod-am-farwolaethau-gwartheg...</td>\n",
       "      <td>5fa585e4-7631-11e4-a3cb-005056011aef</td>\n",
       "      <td>mae'n rhaid i ladd-dai roi gwybod am symudiada...</td>\n",
       "      <td>detailed_guide</td>\n",
       "      <td>2016-04-28T10:18:35.000+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>British Cattle Movement Service</td>\n",
       "      <td>whitehall</td>\n",
       "      <td>rhoi gwybod am farwolaethau gwartheg ar adeg e...</td>\n",
       "      <td>mae’n rhaid i ladd dai wneud y canlynol: rhoi ...</td>\n",
       "      <td>rhoi gwybod am farwolaethau gwartheg ar adeg e...</td>\n",
       "      <td>322251ef-586c-47df-8869-fa73b6e3aa7c</td>\n",
       "      <td>/environment/keeping-farmed-animals-cattle-dea...</td>\n",
       "      <td>Cymraeg</td>\n",
       "      <td>Environment</td>\n",
       "      <td>Food and farming</td>\n",
       "      <td>Keeping farmed animals</td>\n",
       "      <td>Cattle deaths</td>\n",
       "      <td>Cymraeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285095</th>\n",
       "      <td>/guidance/rhoi-gwybod-am-farwolaethau-gwartheg...</td>\n",
       "      <td>5fa56edb-7631-11e4-a3cb-005056011aef</td>\n",
       "      <td>pan fydd gwartheg yn marw ar y fferm neu ar sa...</td>\n",
       "      <td>detailed_guide</td>\n",
       "      <td>2016-04-28T10:18:35.000+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>British Cattle Movement Service</td>\n",
       "      <td>whitehall</td>\n",
       "      <td>rhoi gwybod am farwolaethau gwartheg ar y dali...</td>\n",
       "      <td>mae’n rhaid i chi roi gwybod am unrhyw wartheg...</td>\n",
       "      <td>rhoi gwybod am farwolaethau gwartheg ar y dali...</td>\n",
       "      <td>322251ef-586c-47df-8869-fa73b6e3aa7c</td>\n",
       "      <td>/environment/keeping-farmed-animals-cattle-dea...</td>\n",
       "      <td>Cymraeg</td>\n",
       "      <td>Environment</td>\n",
       "      <td>Food and farming</td>\n",
       "      <td>Keeping farmed animals</td>\n",
       "      <td>Cattle deaths</td>\n",
       "      <td>Cymraeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285096</th>\n",
       "      <td>/guidance/british-forces-post-office-services</td>\n",
       "      <td>5c8f52ae-7631-11e4-a3cb-005056011aef</td>\n",
       "      <td>how to use the british forces post office clai...</td>\n",
       "      <td>detailed_guide</td>\n",
       "      <td>2012-12-12T19:27:00.000+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>Ministry of Defence</td>\n",
       "      <td>whitehall</td>\n",
       "      <td>bfpo services guide</td>\n",
       "      <td>follow our short guide to find out how to send...</td>\n",
       "      <td>bfpo services guide how to use the british for...</td>\n",
       "      <td>ffedc568-ce84-43f6-b2a8-d749f24c13c7</td>\n",
       "      <td>/defence/defence-armed-forces-support-services...</td>\n",
       "      <td>British Forces Post Office</td>\n",
       "      <td>Defence</td>\n",
       "      <td>Support services for military and defence pers...</td>\n",
       "      <td>British Forces Post Office</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                base_path  \\\n",
       "285092       /guidance/tonnage-tax-for-shipping-companies   \n",
       "285093  /government/collections/infectious-diseases-in...   \n",
       "285094  /guidance/rhoi-gwybod-am-farwolaethau-gwartheg...   \n",
       "285095  /guidance/rhoi-gwybod-am-farwolaethau-gwartheg...   \n",
       "285096      /guidance/british-forces-post-office-services   \n",
       "\n",
       "                                  content_id  \\\n",
       "285092  601f0b66-7631-11e4-a3cb-005056011aef   \n",
       "285093  bf520108-e372-4a6e-ad8d-9e7ed7bd065d   \n",
       "285094  5fa585e4-7631-11e4-a3cb-005056011aef   \n",
       "285095  5fa56edb-7631-11e4-a3cb-005056011aef   \n",
       "285096  5c8f52ae-7631-11e4-a3cb-005056011aef   \n",
       "\n",
       "                                              description  \\\n",
       "285092  understand how you can pay tonnage tax if you ...   \n",
       "285093  information for healthcare professionals provi...   \n",
       "285094  mae'n rhaid i ladd-dai roi gwybod am symudiada...   \n",
       "285095  pan fydd gwartheg yn marw ar y fferm neu ar sa...   \n",
       "285096  how to use the british forces post office clai...   \n",
       "\n",
       "              document_type             first_published_at locale  \\\n",
       "285092       detailed_guide  2011-12-13T00:00:00.000+00:00     en   \n",
       "285093  document_collection  2016-03-02T15:04:00.000+00:00     en   \n",
       "285094       detailed_guide  2016-04-28T10:18:35.000+00:00     en   \n",
       "285095       detailed_guide  2016-04-28T10:18:35.000+00:00     en   \n",
       "285096       detailed_guide  2012-12-12T19:27:00.000+00:00     en   \n",
       "\n",
       "        primary_publishing_organisation publishing_app  \\\n",
       "285092             HM Revenue & Customs      whitehall   \n",
       "285093            Public Health England      whitehall   \n",
       "285094  British Cattle Movement Service      whitehall   \n",
       "285095  British Cattle Movement Service      whitehall   \n",
       "285096              Ministry of Defence      whitehall   \n",
       "\n",
       "                                                    title  \\\n",
       "285092  find out how to pay tonnage tax if you're a sh...   \n",
       "285093  infectious diseases in pregnancy screening: cl...   \n",
       "285094  rhoi gwybod am farwolaethau gwartheg ar adeg e...   \n",
       "285095  rhoi gwybod am farwolaethau gwartheg ar y dali...   \n",
       "285096                                bfpo services guide   \n",
       "\n",
       "                                                     body  \\\n",
       "285092  overview tonnage tax is a form of corporation ...   \n",
       "285093  these documents explain the procedures for pro...   \n",
       "285094  mae’n rhaid i ladd dai wneud y canlynol: rhoi ...   \n",
       "285095  mae’n rhaid i chi roi gwybod am unrhyw wartheg...   \n",
       "285096  follow our short guide to find out how to send...   \n",
       "\n",
       "                                            combined_text  \\\n",
       "285092  find out how to pay tonnage tax if you're a sh...   \n",
       "285093  infectious diseases in pregnancy screening: cl...   \n",
       "285094  rhoi gwybod am farwolaethau gwartheg ar adeg e...   \n",
       "285095  rhoi gwybod am farwolaethau gwartheg ar y dali...   \n",
       "285096  bfpo services guide how to use the british for...   \n",
       "\n",
       "                                    taxon_id  \\\n",
       "285092  0f0ba370-1824-4326-85a3-c4038a0a3783   \n",
       "285093  1fe05c70-0bd9-4725-9ed4-3abc0f5f7961   \n",
       "285094  322251ef-586c-47df-8869-fa73b6e3aa7c   \n",
       "285095  322251ef-586c-47df-8869-fa73b6e3aa7c   \n",
       "285096  ffedc568-ce84-43f6-b2a8-d749f24c13c7   \n",
       "\n",
       "                                          taxon_base_path  \\\n",
       "285092     /money/business-tax-international-tax-shipping   \n",
       "285093  /health-and-social-care/population-screening-p...   \n",
       "285094  /environment/keeping-farmed-animals-cattle-dea...   \n",
       "285095  /environment/keeping-farmed-animals-cattle-dea...   \n",
       "285096  /defence/defence-armed-forces-support-services...   \n",
       "\n",
       "                             taxon_name             level1taxon  \\\n",
       "285092                         Shipping                   Money   \n",
       "285093  Commission and provide services  Health and social care   \n",
       "285094                          Cymraeg             Environment   \n",
       "285095                          Cymraeg             Environment   \n",
       "285096       British Forces Post Office                 Defence   \n",
       "\n",
       "                                              level2taxon  \\\n",
       "285092                                       Business tax   \n",
       "285093                    Population screening programmes   \n",
       "285094                                   Food and farming   \n",
       "285095                                   Food and farming   \n",
       "285096  Support services for military and defence pers...   \n",
       "\n",
       "                                              level3taxon  \\\n",
       "285092                                  International tax   \n",
       "285093  NHS infectious diseases in pregnancy screening...   \n",
       "285094                             Keeping farmed animals   \n",
       "285095                             Keeping farmed animals   \n",
       "285096                         British Forces Post Office   \n",
       "\n",
       "                            level4taxon level5taxon  \n",
       "285092                         Shipping         NaN  \n",
       "285093  Commission and provide services         NaN  \n",
       "285094                    Cattle deaths     Cymraeg  \n",
       "285095                    Cattle deaths     Cymraeg  \n",
       "285096                              NaN         NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qggWJCvvqfD9",
    "outputId": "64637a19-b497-4d27-8499-5ea42b3e1ce0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxon_id_to_base_path = dict(zip(labelled['taxon_id'], labelled['taxon_base_path']))\n",
    "type(taxon_id_to_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7eEvWttRt7bm",
    "outputId": "fc6ca374-6408-4512-d147-77f86dabc487"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187073"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled.content_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled['brexit'] = np.where(labelled['level2taxon']=='Brexit', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284604\n",
       "1       493\n",
       "Name: brexit, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled.brexit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5XyabI6MgVKe"
   },
   "outputs": [],
   "source": [
    "corpus_sample = labelled.sample(n=20000, random_state=1234)\n",
    "corpus = corpus_sample['combined_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_corpus=[]\n",
    "for text in corpus:\n",
    "    words = text.split()\n",
    "    truncated = \" \".join(words[0:200])\n",
    "    short_corpus.append(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sample['brexit'] = np.where(corpus_sample['level2taxon']=='Brexit', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    19964\n",
       "1       36\n",
       "Name: brexit, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample.brexit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I07gVD3zjuQr"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(LOG_DIR,'metadata.tsv'),'w') as f:\n",
    "    f.write(\"Index\\tTitle\\tTaxon1\\tTaxon2\\tbrexit\\n\")\n",
    "    for index, row in corpus_sample.iterrows():\n",
    "        f.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(index,row['title'], row['level1taxon'],row['level2taxon'], row['brexit']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KghpWC4NKTdf",
    "outputId": "b583daaf-c053-4502-fdac-e040a4eb319f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /var/folders/jy/47p744c95hz67738zkn74rwr0002j9/T/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c6fae0d684cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOG_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model2.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved in path: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#   for i, train_embedding in enumerate(np.array(train_embeddings).tolist()):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.Module(module_url)\n",
    "\n",
    "# Reduce logging output.\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    embedded_sentences = session.run(embed(short_corpus))\n",
    "#     session.run(embed(corpus))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # for tensorboard\n",
    "    emb = tf.Variable(embedded_sentences, name='embedded_sentences')\n",
    "    sess.run(emb.initializer)\n",
    "    config = projector.ProjectorConfig()\n",
    "    summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = emb.name\n",
    "\n",
    "    # Comment out if you don't have metadata\n",
    "    embedding.metadata_path = os.path.join('metadata.tsv')\n",
    "\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "    saver = tf.train.Saver([emb])\n",
    "    saver.save(sess, os.path.join(LOG_DIR, 'model2.ckpt'), 1)\n",
    "    print(\"Model saved in path: %s\" % os.path.join(LOG_DIR, 'model2.ckpt'))\n",
    "\n",
    "#   for i, train_embedding in enumerate(np.array(train_embeddings).tolist()):\n",
    "#     print(\"Text: {}\".format(train_corpus[i]))\n",
    "#     print(\"Embedding size: {}\".format(len(train_embedding)))\n",
    "#     train_embedding_snippet = \", \".join(\n",
    "#         (str(x) for x in train_embedding[:3]))\n",
    "#     print(\"Embedding: [{}, ...]\\n\".format(train_embedding_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "def model_summary():\n",
    "    model_vars = tf.trainable_variables()\n",
    "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
    "\n",
    "model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(embedded_sentences.shape)\n",
    "tsne_emb = TSNE(n_components=2).fit_transform(embedded_sentences)\n",
    "tsne_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(tsne_emb[:, 0], tsne_emb[:, 1],s=0.1)\n",
    "\n",
    "\n",
    "for i, txt in enumerate(corpus_sample['title']):\n",
    "    if (i % 50 == 0):\n",
    "        ax.annotate(txt, (tsne_emb[:, 0][i], tsne_emb[:, 1][i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tsne_df = pd.DataFrame({'dimension1':tsne_emb[:,0],'dimension2':tsne_emb[:,1], 'taxon':corpus_sample['taxon_name']})\n",
    "import seaborn as sns\n",
    " \n",
    "# Use the 'hue' argument to provide a factor variable\n",
    "sns.lmplot( x=\"dimension1\", y=\"dimension2\", data=tsne_df, fit_reg=False, hue='taxon', legend=False)\n",
    "\n",
    " \n",
    "# Move the legend tout of the plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    " \n",
    "#sns.plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2).fit_transform(embedded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame({'dimension1':pca[:,0],'dimension2':pca[:,1], 'taxon':corpus_sample['taxon_name']})\n",
    " \n",
    "# Use the 'hue' argument to provide a factor variable\n",
    "sns.lmplot( x=\"dimension1\", y=\"dimension2\", data=pca_df, fit_reg=False, hue='taxon', legend=False)\n",
    "\n",
    " \n",
    "# Move the legend tout of the plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    " \n",
    "#sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mIap2t_6f4B9"
   },
   "outputs": [],
   "source": [
    "# Create some variables.\n",
    "# emb = tf.Variable(embeddings, name='word_embeddings')\n",
    "# # Save the variables to disk.\n",
    "# save_path = saver.save(session, \"model_dir/model.ckpt\")\n",
    "# print(\"Model saved in path: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WlOCwiEoND2U"
   },
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation):\n",
    "  corr = np.inner(features, features)\n",
    "  sns.set(font_scale=1.2)\n",
    "  g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "  g.set_xticklabels(labels, rotation=rotation)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "\n",
    "def run_and_plot(session_, input_tensor_, messages_, encoding_tensor):\n",
    "  message_embeddings_ = session_.run(\n",
    "      encoding_tensor, feed_dict={input_tensor_: messages_})\n",
    "  plot_similarity(messages_, message_embeddings_, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hbDbSKfCNFWV"
   },
   "outputs": [],
   "source": [
    "# similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "# similarity_encodings = embed(similarity_input_placeholder)\n",
    "# with tf.Session() as session:\n",
    "#   session.run(tf.global_variables_initializer())\n",
    "#   session.run(tf.tables_initializer())\n",
    "#   run_and_plot(session, similarity_input_placeholder, train_corpus,\n",
    "#                similarity_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TFSLcBy_XhCj"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vdo6CwWeeOwT"
   },
   "outputs": [],
   "source": [
    "cosine(train_embeddings[0], train_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2UUS5IneYu0i"
   },
   "outputs": [],
   "source": [
    "def find_most_similar(index_document_embedding, all_document_embeddings):\n",
    "  lowest_cosine = np.float(1)\n",
    "  nearest_index = 0\n",
    "  for index, document_embedding in enumerate(all_document_embeddings):\n",
    "    cos = cosine(index_document_embedding, document_embedding)\n",
    "    \n",
    "    if cos < lowest_cosine:\n",
    "      lowest_cosine = cos\n",
    "      nearest_index = index\n",
    "      \n",
    "      \n",
    "  return lowest_cosine, nearest_index\n",
    "\n",
    "found_themselves_nearest =[]\n",
    "for first_index, document in enumerate(train_embeddings):\n",
    "  cos, second_index = find_most_similar(document, train_embeddings)\n",
    "  print(first_index, second_index)\n",
    "  found_itself_nearest = int(np.where(first_index==second_index, 1, 0))\n",
    "  found_themselves_nearest.append(found_itself_nearest)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hjKi4LRCvXOl"
   },
   "outputs": [],
   "source": [
    "labelled_sample = labelled.sample(n=10)\n",
    "\n",
    "labelled_corpus = labelled['combined_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ewwwYsduyz-"
   },
   "outputs": [],
   "source": [
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.Module(module_url)\n",
    "\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "  embeddings = session.run(embed(labelled_corpus))\n",
    "\n",
    "  for i, train_embedding in enumerate(np.array(embeddings).tolist()):\n",
    "    print(\"Text: {}\".format(train_corpus[i]))\n",
    "    print(\"Embedding size: {}\".format(len(train_embedding)))\n",
    "    train_embedding_snippet = \", \".join(\n",
    "        (str(x) for x in train_embedding[:3]))\n",
    "    print(\"Embedding: [{}, ...]\\n\".format(train_embedding_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfqZrZxRw0Q5"
   },
   "outputs": [],
   "source": [
    "taxons = corpus_sample['taxon_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OcgvHrDcnJW"
   },
   "outputs": [],
   "source": [
    "taxon_embeddings = embeddings[corpus_sample['taxon_id']=='1327984f-95e0-4ca7-94c7-c63e69c30924']\n",
    "\n",
    "cosine_results = []\n",
    "for i in taxon_embeddings:\n",
    "  for j in taxon_embeddings:\n",
    "    cosine_results.append(cosine(i, j))\n",
    "mean_cosine_for_taxon = np.mean(np.array(cosine_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9zT2rUXk2Dh"
   },
   "outputs": [],
   "source": [
    "mean_cosine_for_taxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VCQ8Bp6rw70Y"
   },
   "outputs": [],
   "source": [
    "taxon_homogeneity = []\n",
    "for taxon in progressbar.progressbar(taxons):\n",
    "  taxon_embeddings = embeddings[corpus_sample['taxon_id']==taxon]\n",
    "  taxon_size = taxon_embeddings.shape[0]\n",
    "  cosine_results = []\n",
    "  for i in taxon_embeddings:\n",
    "    for j in taxon_embeddings:\n",
    "      cosine_results.append(cosine(i, j))\n",
    "  mean_cosine_for_taxon = np.mean(np.array(cosine_results))\n",
    "  \n",
    "\n",
    "  taxon_homogeneity.append([taxon, taxon_size, mean_cosine_for_taxon])\n",
    "      \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LxQbi83m7RC"
   },
   "outputs": [],
   "source": [
    "taxon_homogeneity_df = pd.DataFrame(taxon_homogeneity, columns = ['taxon_id', 'taxon_size', 'mean_cosine_score']).sort_values('mean_cosine_score', ascending=False)\n",
    "taxon_homogeneity_df['taxon_base_path'] = taxon_homogeneity_df['taxon_id'].map(taxon_id_to_base_path)\n",
    "taxon_homogeneity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z_eD75-8wnuz"
   },
   "outputs": [],
   "source": [
    "alt.Chart(taxon_homogeneity_df).mark_circle(size=60).encode(\n",
    "    x='taxon_size',\n",
    "    y='mean_cosine_score',\n",
    "    tooltip=['taxon_base_path']\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "77HvQ0ZUHGAR"
   },
   "source": [
    "# Inferring a Vector\n",
    "One important thing to note is that you can now infer a vector for any piece of text without having to re-train the model by passing a list of words to the model.infer_vector function. This vector can then be compared with other vectors via cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJynSrzRHGAR"
   },
   "outputs": [],
   "source": [
    "print(model.infer_vector(train_corpus[0].words))\n",
    "print(train_corpus[0].tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87Fex-_fHGAV"
   },
   "source": [
    "Note that infer_vector() does not take a string, but rather a list of string tokens, which should have already been tokenized the same way as the words property of original training document objects.\n",
    "\n",
    "Also note that because the underlying training/inference algorithms are an iterative approximation problem that makes use of internal randomization, repeated inferences of the same text will return slightly different vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyPQmQ6hHGAV"
   },
   "source": [
    "# Assessing Model\n",
    "To assess our new model, we'll first infer new vectors for each document of the training corpus, compare the inferred vectors with the training corpus, and then returning the rank of the document based on self-similarity. Basically, we're pretending as if the training corpus is some new unseen data and then seeing how they compare with the trained model. The expectation is that we've likely overfit our model (i.e., all of the ranks will be less than 2) and so we should be able to find similar documents very easily. Additionally, we'll keep track of the second ranks for a comparison of less similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ay4Q8IwgHGAW"
   },
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "\n",
    "for doc_id in progressbar.progressbar(range(len(train_corpus))):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    found_itself_nearest = int(np.where(sims[0][0]==train_corpus[doc_id].tags[0], 1, 0))\n",
    "#     rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(found_itself_nearest)\n",
    "    \n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "asEJr35THGAa"
   },
   "source": [
    "Let's count how each document ranks with respect to the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sNu7IFs8HGAb"
   },
   "outputs": [],
   "source": [
    "collections.Counter(ranks)  # Results vary between runs due to random seeding and very small corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xboHqp0lHGAg"
   },
   "outputs": [],
   "source": [
    "147620/162461*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-BeXIYVHGAi"
   },
   "source": [
    "Basically, 91% of the inferred documents are found to be most similar to itself and 9% it is mistakenly most similar to another document. the checking of an inferred-vector against a training-vector is a sort of 'sanity check' as to whether the model is behaving in a usefully consistent manner, though not a real 'accuracy' value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33nfiaTKHGAj"
   },
   "outputs": [],
   "source": [
    "len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCk3DG3kHGAm"
   },
   "outputs": [],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print('({},{}),{})'.format(label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BRVbgbXNHGAp"
   },
   "source": [
    "#### Sampling docs to get a faster measure of global auto-similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jR08cZijHGAp"
   },
   "source": [
    "##### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVbQXidDHGAq"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(train_corpus):\n",
    "    train_auto_nearest = []\n",
    "    random.seed(1234)\n",
    "    sample_1000 = random.sample(train_corpus, 1000)\n",
    "\n",
    "    for doc_id in progressbar.progressbar(range(len(sample_1000))):\n",
    "        inferred_vector = model.infer_vector(sample_1000[doc_id].words)\n",
    "        sims = model.docvecs.most_similar([inferred_vector], topn=2)\n",
    "        found_itself_nearest = int(np.where(sims[0][0]==sample_1000[doc_id].tags[0], 1, 0))\n",
    "        train_auto_nearest.append(found_itself_nearest)\n",
    "    \n",
    "    \n",
    "    x = collections.Counter(train_auto_nearest)\n",
    "    train_percent_auto_similar = x[1]/(x[0]+x[1])*100\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"The percentage of 1000 training samples which found itself nearest = {}\".format(train_percent_auto_similar\n",
    "                                                                                          )\n",
    "         )\n",
    "    return train_percent_auto_similar \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kXAf3qZDHGAs"
   },
   "outputs": [],
   "source": [
    "evaluate_model(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZbWs-pZHGAv"
   },
   "source": [
    "~91% auto-similarity in the sample of 1000. Think this is a viable approach for measuring models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNnqXnCIHGAw"
   },
   "source": [
    "# Testing the Model\n",
    "Using the same approach above, we'll infer the vector for a randomly chosen test document, and compare the document to our model by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJPVD-xsHGAw"
   },
   "outputs": [],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITNTBhXuHGAz"
   },
   "outputs": [],
   "source": [
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=10)\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND', 1), ('THIRD', 2), ('FOURTH', 3), ('FIFTH', 4)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYJcp4R0HGA2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "similarity_universal_sentence_embedding.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
