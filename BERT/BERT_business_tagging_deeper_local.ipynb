{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tf_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cp5wfXDx5SPH"
   },
   "source": [
    "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7293,
     "status": "ok",
     "timestamp": 1551188318055,
     "user": {
      "displayName": "Ellie King",
      "photoUrl": "https://lh6.googleusercontent.com/-qFoTphed8uc/AAAAAAAAAAI/AAAAAAAAAA8/6J_AqIipvA0/s64/photo.jpg",
      "userId": "04123326866714944866"
     },
     "user_tz": 0
    },
    "id": "jviywGyWyKsA",
    "outputId": "398cc633-6e1c-4ac4-a684-2c66b06fe42f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /Users/ellieking/.pyenv/versions/3.6.4/envs/content-similarity-3.6.4/lib/python3.6/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /Users/ellieking/.pyenv/versions/3.6.4/envs/content-similarity-3.6.4/lib/python3.6/site-packages (from bert-tensorflow) (1.11.0)\n",
      "\u001b[33mYou are using pip version 19.0.2, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhbGEfwgdEtw"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVB3eOcjxxm1"
   },
   "source": [
    "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
    "\n",
    "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
    "\n",
    "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 741,
     "status": "ok",
     "timestamp": 1551188369570,
     "user": {
      "displayName": "Ellie King",
      "photoUrl": "https://lh6.googleusercontent.com/-qFoTphed8uc/AAAAAAAAAAI/AAAAAAAAAA8/6J_AqIipvA0/s64/photo.jpg",
      "userId": "04123326866714944866"
     },
     "user_tz": 0
    },
    "id": "US_EAnICvP7f",
    "outputId": "71976766-674d-45b5-9f2a-dfa67dc9ee97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: /Users/ellieking/Documents/content-similarity/BERT/bert_experiment_output *****\n"
     ]
    }
   ],
   "source": [
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "\n",
    "OUTPUT_DIR = \"/Users/ellieking/Documents/content-similarity/BERT/bert_experiment_output\"#@param {type:\"string\"}\n",
    "#@markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = False #@param {type:\"boolean\"}\n",
    "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
    "USE_BUCKET = False #@param {type:\"boolean\"}\n",
    "BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n",
    "\n",
    "if USE_BUCKET:\n",
    "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "\n",
    "if DO_DELETE:\n",
    "  try:\n",
    "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "  except:\n",
    "    # Doesn't matter if the directory didn't exist\n",
    "    pass\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmFYvkylMwXn"
   },
   "source": [
    "#Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ellieking/Documents/content-similarity/BERT/bert_experiment_output'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MC_w8SRqN0fr"
   },
   "source": [
    "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbOjC5JIEPi9"
   },
   "outputs": [],
   "source": [
    "labelled = pd.read_csv('/Users/ellieking/Documents/govuk-taxonomy-supervised-learning/data/2019-02-11/labelled.csv.gz', \n",
    "                       compression='gzip', \n",
    "                       low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NUFtddr8GbPX"
   },
   "outputs": [],
   "source": [
    "business = labelled[labelled['level1taxon']=='Business and industry'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1004,
     "status": "ok",
     "timestamp": 1551189582964,
     "user": {
      "displayName": "Ellie King",
      "photoUrl": "https://lh6.googleusercontent.com/-qFoTphed8uc/AAAAAAAAAAI/AAAAAAAAAA8/6J_AqIipvA0/s64/photo.jpg",
      "userId": "04123326866714944866"
     },
     "user_tz": 0
    },
    "id": "-nm-6U-HGo_a",
    "outputId": "2ef20d28-ce7f-42b0-a5d0-9da307220603"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52715, 19)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztbG__5tNIFp"
   },
   "outputs": [],
   "source": [
    "business = business.assign(level=np.where(business.level5taxon.notnull(), 5, 0))\n",
    "business.loc[business['level4taxon'].notnull() & business['level5taxon'].isnull(\n",
    "), 'level'] = 4\n",
    "business.loc[business['level3taxon'].notnull() & business['level4taxon'].isnull(\n",
    "), 'level'] = 3\n",
    "business.loc[business['level2taxon'].notnull() & business['level3taxon'].isnull(\n",
    "), 'level'] = 2\n",
    "business.loc[business['level1taxon'].notnull() & business['level2taxon'].isnull(\n",
    "), 'level'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fyCKc8i2Nieg"
   },
   "outputs": [],
   "source": [
    "deep = business[business['level']>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1551191442031,
     "user": {
      "displayName": "Ellie King",
      "photoUrl": "https://lh6.googleusercontent.com/-qFoTphed8uc/AAAAAAAAAAI/AAAAAAAAAA8/6J_AqIipvA0/s64/photo.jpg",
      "userId": "04123326866714944866"
     },
     "user_tz": 0
    },
    "id": "T4yTE85wNsBM",
    "outputId": "d11990e4-327f-4caa-af71-066032e0dde3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21363, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep.to_csv('/Users/ellieking/Documents/govuk-taxonomy-supervised-learning/data/2019-02-11/business_deep.csv.gz', \n",
    "            compression='gzip', \n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2abfwdn-g135"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(deep, test_size=0.33, random_state=42, stratify=deep['level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA8WHJgzhIZf"
   },
   "source": [
    "To keep training fast, we'll take a sample of 5000 train and test examples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1551191512814,
     "user": {
      "displayName": "Ellie King",
      "photoUrl": "https://lh6.googleusercontent.com/-qFoTphed8uc/AAAAAAAAAAI/AAAAAAAAAA8/6J_AqIipvA0/s64/photo.jpg",
      "userId": "04123326866714944866"
     },
     "user_tz": 0
    },
    "id": "prRQM8pDi8xI",
    "outputId": "74e803d5-f9c5-4ab2-f1a6-cae457777c46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['base_path', 'content_id', 'description', 'document_type',\n",
       "       'first_published_at', 'locale', 'primary_publishing_organisation',\n",
       "       'publishing_app', 'title', 'body', 'combined_text', 'taxon_id',\n",
       "       'taxon_base_path', 'taxon_name', 'level1taxon', 'level2taxon',\n",
       "       'level3taxon', 'level4taxon', 'level5taxon', 'level'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfRnHSz3iSXz"
   },
   "source": [
    "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'combined_text'\n",
    "LABEL_COLUMN = 'taxon_id'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = list(deep.taxon_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.taxon_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9413"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.content_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14313, 20)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.taxon_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V399W0rqNJ-Z"
   },
   "source": [
    "#Data Preprocessing\n",
    "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
    "\n",
    "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
    "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
    "- `label` is the label for our example, i.e. True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9gEt5SmM6i6"
   },
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCZWZtKxObjh"
   },
   "source": [
    "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "\n",
    "\n",
    "1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "4. Map our words to indexes using a vocab file that BERT provides\n",
    "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "\n",
    "Happily, we don't have to worry about most of these details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMWiDtpyQSoU"
   },
   "source": [
    "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15344,
     "status": "ok",
     "timestamp": 1551191585315,
     "user": {
      "displayName": "Ellie King",
      "photoUrl": "https://lh6.googleusercontent.com/-qFoTphed8uc/AAAAAAAAAAI/AAAAAAAAAA8/6J_AqIipvA0/s64/photo.jpg",
      "userId": "04123326866714944866"
     },
     "user_tz": 0
    },
    "id": "IhJSe0QHNG7U",
    "outputId": "5dc49154-336f-434f-a436-0dff7f52604e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /var/folders/jy/47p744c95hz67738zkn74rwr0002j9/T/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4oFkhpZBDKm"
   },
   "source": [
    "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OEzfFIt6GIc"
   },
   "source": [
    "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1278
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 233222,
     "status": "ok",
     "timestamp": 1551191860095,
     "user": {
      "displayName": "Ellie King",
      "photoUrl": "https://lh6.googleusercontent.com/-qFoTphed8uc/AAAAAAAAAAI/AAAAAAAAAA8/6J_AqIipvA0/s64/photo.jpg",
      "userId": "04123326866714944866"
     },
     "user_tz": 0
    },
    "id": "LL5W8gEGRTAf",
    "outputId": "d350c591-59ad-4593-d94a-43797164339a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 14313\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] hbo ##s / david lloyd leisure ltd dl ##l sa david lloyd leisure espana i sl david lloyd leisure espana ii sl next generation clubs holdings ltd of ##t closed case : completed acquisition by hbo ##s plc of control of david lloyd leisure ltd dl ##l sa david lloyd leisure espana i sl david lloyd leisure espana ii sl and next generation clubs holdings ltd . affected market : health and fitness clubs no . me / 328 ##5 / 07 the of ##t ’ s decision on reference under section 22 ( 1 ) given on 5 november 2007 . full text of decision published 16 november 2007 . please note that square brackets indicate figures which have been deleted or replaced with a [SEP]\n",
      "INFO:tensorflow:input_ids: 101 14633 2015 1013 2585 6746 12257 5183 21469 2140 7842 2585 6746 12257 20678 1045 22889 2585 6746 12257 20678 2462 22889 2279 4245 4184 9583 5183 1997 2102 2701 2553 1024 2949 7654 2011 14633 2015 15492 1997 2491 1997 2585 6746 12257 5183 21469 2140 7842 2585 6746 12257 20678 1045 22889 2585 6746 12257 20678 2462 22889 1998 2279 4245 4184 9583 5183 1012 5360 3006 1024 2740 1998 10516 4184 2053 1012 2033 1013 25256 2629 1013 5718 1996 1997 2102 1521 1055 3247 2006 4431 2104 2930 2570 1006 1015 1007 2445 2006 1019 2281 2289 1012 2440 3793 1997 3247 2405 2385 2281 2289 1012 3531 3602 2008 2675 19719 5769 4481 2029 2031 2042 17159 2030 2999 2007 1037 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: a3773f5e-f05e-48b0-b79a-46374dabbc30 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] pm ' s speech to the cb ##i conference : 6 november 2017 prime minister theresa may addressed the confederation of british industry ' s 2017 annual conference with a speech titled ' getting it right for the long - term ' . thank you and it is a pleasure to be here . last year i spoke to you about my belief in a well regulated free market economy . i said it was the very best way to spread opportunity and lift people out of poverty . we should never under ##est ##imate the immense value and potential of open innovative free market economies when they operate under the right rules and regulations . around the world over the last century it has been [SEP]\n",
      "INFO:tensorflow:input_ids: 101 7610 1005 1055 4613 2000 1996 17324 2072 3034 1024 1020 2281 2418 3539 2704 14781 2089 8280 1996 11078 1997 2329 3068 1005 1055 2418 3296 3034 2007 1037 4613 4159 1005 2893 2009 2157 2005 1996 2146 1011 2744 1005 1012 4067 2017 1998 2009 2003 1037 5165 2000 2022 2182 1012 2197 2095 1045 3764 2000 2017 2055 2026 6772 1999 1037 2092 12222 2489 3006 4610 1012 1045 2056 2009 2001 1996 2200 2190 2126 2000 3659 4495 1998 6336 2111 2041 1997 5635 1012 2057 2323 2196 2104 4355 21499 1996 14269 3643 1998 4022 1997 2330 9525 2489 3006 18730 2043 2027 5452 2104 1996 2157 3513 1998 7040 1012 2105 1996 2088 2058 1996 2197 2301 2009 2038 2042 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 54758e13-1f51-4413-833c-70254776b06f (id = 16)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] instant business advice - manchester city free live chat business advice for entrepreneurs start - ups existing businesses and residents in the manchester city council local authority area . who it ’ s for anyone in north of england within the manchester city council local authority area can use this service . what you can get you can get help with issues including : business planning and business regulations market research marketing and sales finance and funding premises employing people live chat is available monday to friday 9 ##am to 7 ##pm . outside of these hours you can leave a message and will receive a response the following working day . the platform also provides opportunities to network with other entrepreneurs . organise ##r mi [SEP]\n",
      "INFO:tensorflow:input_ids: 101 7107 2449 6040 1011 5087 2103 2489 2444 11834 2449 6040 2005 17633 2707 1011 11139 4493 5661 1998 3901 1999 1996 5087 2103 2473 2334 3691 2181 1012 2040 2009 1521 1055 2005 3087 1999 2167 1997 2563 2306 1996 5087 2103 2473 2334 3691 2181 2064 2224 2023 2326 1012 2054 2017 2064 2131 2017 2064 2131 2393 2007 3314 2164 1024 2449 4041 1998 2449 7040 3006 2470 5821 1998 4341 5446 1998 4804 10345 15440 2111 2444 11834 2003 2800 6928 2000 5958 1023 3286 2000 1021 9737 1012 2648 1997 2122 2847 2017 2064 2681 1037 4471 1998 2097 4374 1037 3433 1996 2206 2551 2154 1012 1996 4132 2036 3640 6695 2000 2897 2007 2060 17633 1012 22933 2099 2771 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: ccfc50f5-e193-4dac-9d78-50b3a8bb24c5 (id = 17)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] ph g ##lat ##felt ##er company / jr cr ##omp ##ton ltd of ##t closed case : anticipated acquisition by ph g ##lat ##felt ##er company of certain assets of jr cr ##omp ##ton limited . affected market : specialist filter paper no . me / 238 ##2 / 06 the of ##t has decided to request that the european commission examine this merger . the request has been made in accordance with article 22 of council regulation ( ec ) no 139 / 2004 ( the ec merger regulation ) . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 6887 1043 20051 26675 2121 2194 1013 3781 13675 25377 2669 5183 1997 2102 2701 2553 1024 11436 7654 2011 6887 1043 20051 26675 2121 2194 1997 3056 7045 1997 3781 13675 25377 2669 3132 1012 5360 3006 1024 8325 11307 3259 2053 1012 2033 1013 22030 2475 1013 5757 1996 1997 2102 2038 2787 2000 5227 2008 1996 2647 3222 11628 2023 7660 1012 1996 5227 2038 2042 2081 1999 10388 2007 3720 2570 1997 2473 7816 1006 14925 1007 2053 16621 1013 2432 1006 1996 14925 7660 7816 1007 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 8c19f8a6-35c0-4e00-98fd-870fe219affd (id = 4)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] prime minister visits nissan sunderland car manufacturing plant the prime minister visits car plant as 100 % electric leaf car goes into production supporting more than 2 000 uk automotive industry jobs . prime minister david cameron is today visiting the nissan car manufacturing plant in sunderland as the company thanked the uk government for continuing support and investment . the visit marks the official start of production of the new 100 % electric nissan leaf . the car is now rolling off the line at the company ’ s sunderland plant using advanced lithium ion batteries manufactured in nissan ’ s new uk battery plant . together the battery plant and nissan leaf production are supporting jobs for more than 2 000 people in the [SEP]\n",
      "INFO:tensorflow:input_ids: 101 3539 2704 7879 16509 15518 2482 5814 3269 1996 3539 2704 7879 2482 3269 2004 2531 1003 3751 7053 2482 3632 2046 2537 4637 2062 2084 1016 2199 2866 12945 3068 5841 1012 3539 2704 2585 7232 2003 2651 5873 1996 16509 2482 5814 3269 1999 15518 2004 1996 2194 15583 1996 2866 2231 2005 5719 2490 1998 5211 1012 1996 3942 6017 1996 2880 2707 1997 2537 1997 1996 2047 2531 1003 3751 16509 7053 1012 1996 2482 2003 2085 5291 2125 1996 2240 2012 1996 2194 1521 1055 15518 3269 2478 3935 22157 10163 10274 7609 1999 16509 1521 1055 2047 2866 6046 3269 1012 2362 1996 6046 3269 1998 16509 7053 2537 2024 4637 5841 2005 2062 2084 1016 2199 2111 1999 1996 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 54758e13-1f51-4413-833c-70254776b06f (id = 16)\n",
      "INFO:tensorflow:Writing example 10000 of 14313\n",
      "INFO:tensorflow:Writing example 0 of 7050\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] avery dennis ##on / mac ##ta ##c merger inquiry the cm ##a investigated the anticipated acquisition by avery dennis ##on of mac ##ta ##c ( europe ) . statutory timetable phase 1 date action 1 september 2016 decision published 26 july 2016 decision announced 13 june to 27 june 2016 invitation to comment 13 june 2016 launch of merger inquiry phase 1 cm ##a clearance decision 26 july 2016 : the cm ##a has cleared the acquisition by avery dennis ##on of mac ##ta ##c ( europe ) . the full text of the decision is available below . full text decision ( 1 . 9 . 16 ) invitation to comment : now closed 13 june 2016 : the cm ##a is considering whether it [SEP]\n",
      "INFO:tensorflow:input_ids: 101 12140 6877 2239 1013 6097 2696 2278 7660 9934 1996 4642 2050 10847 1996 11436 7654 2011 12140 6877 2239 1997 6097 2696 2278 1006 2885 1007 1012 15201 23839 4403 1015 3058 2895 1015 2244 2355 3247 2405 2656 2251 2355 3247 2623 2410 2238 2000 2676 2238 2355 8468 2000 7615 2410 2238 2355 4888 1997 7660 9934 4403 1015 4642 2050 14860 3247 2656 2251 2355 1024 1996 4642 2050 2038 5985 1996 7654 2011 12140 6877 2239 1997 6097 2696 2278 1006 2885 1007 1012 1996 2440 3793 1997 1996 3247 2003 2800 2917 1012 2440 3793 3247 1006 1015 1012 1023 1012 2385 1007 8468 2000 7615 1024 2085 2701 2410 2238 2355 1024 1996 4642 2050 2003 6195 3251 2009 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: a3773f5e-f05e-48b0-b79a-46374dabbc30 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] opt ##imum ph ##asi ##ng of high voltage circuit power lines : voluntary code of practice this voluntary code of practice sets out key principles for the electricity industry to undertake opt ##imum ph ##asi ##ng ( defined in more detail below … this voluntary code of practice sets out key principles for the electricity industry to undertake opt ##imum ph ##asi ##ng ( defined in more detail below ) of all new high voltage ( 132 kv and above ) double circuit power lines and to convert existing power lines where pr ##actic ##able . opt ##imum ph ##asi ##ng of high voltage double - circuit power lines : a voluntary code of practice ref : 12 ##d / 03 ##6 pdf 104 ##k ##b [SEP]\n",
      "INFO:tensorflow:input_ids: 101 23569 28591 6887 21369 3070 1997 2152 10004 4984 2373 3210 1024 10758 3642 1997 3218 2023 10758 3642 1997 3218 4520 2041 3145 6481 2005 1996 6451 3068 2000 16617 23569 28591 6887 21369 3070 1006 4225 1999 2062 6987 2917 1529 2023 10758 3642 1997 3218 4520 2041 3145 6481 2005 1996 6451 3068 2000 16617 23569 28591 6887 21369 3070 1006 4225 1999 2062 6987 2917 1007 1997 2035 2047 2152 10004 1006 14078 24888 1998 2682 1007 3313 4984 2373 3210 1998 2000 10463 4493 2373 3210 2073 10975 28804 3085 1012 23569 28591 6887 21369 3070 1997 2152 10004 3313 1011 4984 2373 3210 1024 1037 10758 3642 1997 3218 25416 1024 2260 2094 1013 6021 2575 11135 9645 2243 2497 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 092c86a5-717e-4bea-be43-4a5d8695a113 (id = 24)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] national film and television school funding settlement letters national film and television school funding settlement letters in line with our commitment to transparency we are publishing the allocation letters the department is sending to its funded bodies so the public can see how their money is being spent and what we expect in return . national film and television school funding allocation letter : issued june 2015 pdf 79 . 6 ##k ##b 2 pages this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email publications @ culture . gs ##i [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2120 2143 1998 2547 2082 4804 4093 4144 2120 2143 1998 2547 2082 4804 4093 4144 1999 2240 2007 2256 8426 2000 16987 2057 2024 4640 1996 16169 4144 1996 2533 2003 6016 2000 2049 6787 4230 2061 1996 2270 2064 2156 2129 2037 2769 2003 2108 2985 1998 2054 2057 5987 1999 2709 1012 2120 2143 1998 2547 2082 4804 16169 3661 1024 3843 2238 2325 11135 6535 1012 1020 2243 2497 1016 5530 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 5523 1030 3226 1012 28177 2072 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: c8f01be4-435b-46d1-b6d2-7fc4cebc0518 (id = 32)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] new rules on video game classification latest move by government to better protect children from unsuitable material selling video games with a “ 12 ” rating to under ##age children will now be a criminal offence punish ##able with a he ##ft ##y fine or even a prison sentence . the new rules are part of a transformation in video game regulation with a simplified ratings system which comes into effect today . video games will now be classified by a single authority the video standards council ( vs ##c ) under the pan european games information ( peg ##i ) system . “ the uk has one of the most dynamic and innovative video games industries in the world and the games they produce not [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2047 3513 2006 2678 2208 5579 6745 2693 2011 2231 2000 2488 4047 2336 2013 25622 3430 4855 2678 2399 2007 1037 1523 2260 1524 5790 2000 2104 4270 2336 2097 2085 2022 1037 4735 15226 16385 3085 2007 1037 2002 6199 2100 2986 2030 2130 1037 3827 6251 1012 1996 2047 3513 2024 2112 1997 1037 8651 1999 2678 2208 7816 2007 1037 11038 8599 2291 2029 3310 2046 3466 2651 1012 2678 2399 2097 2085 2022 6219 2011 1037 2309 3691 1996 2678 4781 2473 1006 5443 2278 1007 2104 1996 6090 2647 2399 2592 1006 25039 2072 1007 2291 1012 1523 1996 2866 2038 2028 1997 1996 2087 8790 1998 9525 2678 2399 6088 1999 1996 2088 1998 1996 2399 2027 3965 2025 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: c8f01be4-435b-46d1-b6d2-7fc4cebc0518 (id = 32)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] advent / priory investments holdings ltd of ##t closed case : anticipated acquisition by advent international corporation of priory investments holdings limited . full text of the decision : advent . pdf [SEP]\n",
      "INFO:tensorflow:input_ids: 101 13896 1013 14284 10518 9583 5183 1997 2102 2701 2553 1024 11436 7654 2011 13896 2248 3840 1997 14284 10518 9583 3132 1012 2440 3793 1997 1996 3247 1024 13896 1012 11135 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 8c19f8a6-35c0-4e00-98fd-870fe219affd (id = 4)\n"
     ]
    }
   ],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[0].label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 14633,\n",
       " 2015,\n",
       " 1013,\n",
       " 2585,\n",
       " 6746,\n",
       " 12257,\n",
       " 5183,\n",
       " 21469,\n",
       " 2140,\n",
       " 7842,\n",
       " 2585,\n",
       " 6746,\n",
       " 12257,\n",
       " 20678,\n",
       " 1045,\n",
       " 22889,\n",
       " 2585,\n",
       " 6746,\n",
       " 12257,\n",
       " 20678,\n",
       " 2462,\n",
       " 22889,\n",
       " 2279,\n",
       " 4245,\n",
       " 4184,\n",
       " 9583,\n",
       " 5183,\n",
       " 1997,\n",
       " 2102,\n",
       " 2701,\n",
       " 2553,\n",
       " 1024,\n",
       " 2949,\n",
       " 7654,\n",
       " 2011,\n",
       " 14633,\n",
       " 2015,\n",
       " 15492,\n",
       " 1997,\n",
       " 2491,\n",
       " 1997,\n",
       " 2585,\n",
       " 6746,\n",
       " 12257,\n",
       " 5183,\n",
       " 21469,\n",
       " 2140,\n",
       " 7842,\n",
       " 2585,\n",
       " 6746,\n",
       " 12257,\n",
       " 20678,\n",
       " 1045,\n",
       " 22889,\n",
       " 2585,\n",
       " 6746,\n",
       " 12257,\n",
       " 20678,\n",
       " 2462,\n",
       " 22889,\n",
       " 1998,\n",
       " 2279,\n",
       " 4245,\n",
       " 4184,\n",
       " 9583,\n",
       " 5183,\n",
       " 1012,\n",
       " 5360,\n",
       " 3006,\n",
       " 1024,\n",
       " 2740,\n",
       " 1998,\n",
       " 10516,\n",
       " 4184,\n",
       " 2053,\n",
       " 1012,\n",
       " 2033,\n",
       " 1013,\n",
       " 25256,\n",
       " 2629,\n",
       " 1013,\n",
       " 5718,\n",
       " 1996,\n",
       " 1997,\n",
       " 2102,\n",
       " 1521,\n",
       " 1055,\n",
       " 3247,\n",
       " 2006,\n",
       " 4431,\n",
       " 2104,\n",
       " 2930,\n",
       " 2570,\n",
       " 1006,\n",
       " 1015,\n",
       " 1007,\n",
       " 2445,\n",
       " 2006,\n",
       " 1019,\n",
       " 2281,\n",
       " 2289,\n",
       " 1012,\n",
       " 2440,\n",
       " 3793,\n",
       " 1997,\n",
       " 3247,\n",
       " 2405,\n",
       " 2385,\n",
       " 2281,\n",
       " 2289,\n",
       " 1012,\n",
       " 3531,\n",
       " 3602,\n",
       " 2008,\n",
       " 2675,\n",
       " 19719,\n",
       " 5769,\n",
       " 4481,\n",
       " 2029,\n",
       " 2031,\n",
       " 2042,\n",
       " 17159,\n",
       " 2030,\n",
       " 2999,\n",
       " 2007,\n",
       " 1037,\n",
       " 102]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[0].input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[0].input_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccp5trMwRtmr"
   },
   "source": [
    "#Creating a model\n",
    "\n",
    "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sequence-level classification tasks, BERT fine-tuning is straightforward. \n",
    "\n",
    "To obtain a fixed-dimensional pooled representation of the input sequence, we take the final hidden state (ie the output of the transformer) for the first token in the input, which by construction corresponds to the special [CLS] word embedding. We denote this vector as $C\\in \\mathbb{R}^{H}$, where $H$ is the hidden size. `output layer`\n",
    "\n",
    "The only new parameters added during fine-tuning are for a classification layer $W \\in \\mathbb{R}^{K x H}$, where $K$ is the number of classification labels.`output_weights`\n",
    "\n",
    "The label probabilities $P \\in \\mathbb{R}^{K}$ are computed with a standard softmax, $P = softmax(CW)^{T}$ `log_probs`\n",
    "\n",
    "All of the parameters of BERT and $W$ `output_weights` are fine-tuned jointly to maximise the log-probability of the correct label. \n",
    "\n",
    "The follwoing hyperparameter vlaues were found to work well across tasks:\n",
    "- *Batch size*: 16, 32 \n",
    "- *Learning rate (Adam)*: 5e-3, 3e-5, 2e-5\n",
    "- *Number of epochs*: 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o2a5ZIvRcJq"
   },
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "  # Use \"sequence_outputs\" for token-level output.\n",
    "  output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "  # Create our own layer to fine-tune.\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    one_hot_predicted_labels = tf.one_hot(predicted_labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs, one_hot_predicted_labels, one_hot_labels)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels, log_probs)\n",
    "#     per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "#     loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs, one_hot_predicted_labels, one_hot_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpE0ZIDOCQzE"
   },
   "source": [
    "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnH-AnOQ9KKW"
   },
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs, one_hot_predicted_labels, one_hot_labels) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(one_hot_labels, one_hot_predicted_labels):\n",
    "\n",
    "        pos_indices = 1   \n",
    "        average = 'micro'\n",
    "\n",
    "        # Tuple of (value, update_op)\n",
    "        precision = tf_metrics.precision(\n",
    "            one_hot_labels, one_hot_predicted_labels, num_labels, pos_indices, average=average)\n",
    "        recall = tf_metrics.recall(\n",
    "            one_hot_labels, one_hot_predicted_labels, num_labels, pos_indices, average=average)\n",
    "        f2 = tf_metrics.fbeta(\n",
    "            one_hot_labels, one_hot_predicted_labels, num_labels, pos_indices, average=average, beta=2)\n",
    "        f1 = tf_metrics.f1(\n",
    "            one_hot_labels, one_hot_predicted_labels, num_labels, pos_indices, average=average)\n",
    "\n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'f2': f2\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs, one_hot_predicted_labels, one_hot_labels) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels,\n",
    "          'y_true': one_hot_labels,\n",
    "          'y_pred': one_hot_predicted_labels,\n",
    "\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjwJ4bTeWXD8"
   },
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emHf9GhfWBZ_"
   },
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEJldMr3WYZa"
   },
   "outputs": [],
   "source": [
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1551192592062,
     "user": {
      "displayName": "Ellie King",
      "photoUrl": "https://lh6.googleusercontent.com/-qFoTphed8uc/AAAAAAAAAAI/AAAAAAAAAA8/6J_AqIipvA0/s64/photo.jpg",
      "userId": "04123326866714944866"
     },
     "user_tz": 0
    },
    "id": "q_WebpS1X97v",
    "outputId": "7fa2b131-b282-4b26-ef34-b2262eda0f3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/ellieking/Documents/content-similarity/BERT/bert_experiment_output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1722289b0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOO3RfG1DYLo"
   },
   "source": [
    "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Pv2bAlOX_-K"
   },
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6Nukby2EB6-"
   },
   "source": [
    "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4536
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26,
     "status": "error",
     "timestamp": 1551193156322,
     "user": {
      "displayName": "Ellie King",
      "photoUrl": "https://lh6.googleusercontent.com/-qFoTphed8uc/AAAAAAAAAAI/AAAAAAAAAA8/6J_AqIipvA0/s64/photo.jpg",
      "userId": "04123326866714944866"
     },
     "user_tz": 0
    },
    "id": "nucD4gluYJmK",
    "outputId": "57d18cc2-b915-408f-86ce-b6593305e3f5"
   },
   "outputs": [],
   "source": [
    "# print(f'Beginning Training!')\n",
    "# current_time = datetime.now()\n",
    "# estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "# print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmbLTVniARy3"
   },
   "source": [
    "Now let's use our test data to see how well our model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIhejfpyJ8Bx"
   },
   "outputs": [],
   "source": [
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ellieking/.pyenv/versions/3.6.4/envs/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-f6cdcd6190a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         (scaffold, update_op, eval_dict, all_hooks) = (\n\u001b[0;32m--> 460\u001b[0;31m             self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n\u001b[0m\u001b[1;32m    461\u001b[0m         return self._evaluate_run(\n\u001b[1;32m    462\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate_build_graph\u001b[0;34m(self, input_fn, hooks, checkpoint_path)\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m       (scaffold, evaluation_hooks, input_hooks, update_op, eval_dict) = (\n\u001b[0;32m-> 1484\u001b[0;31m           self._call_model_fn_eval(input_fn, self.config))\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn_eval\u001b[0;34m(self, input_fn, config)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m     estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1520\u001b[0;31m         features, labels, model_fn_lib.ModeKeys.EVAL, config)\n\u001b[0m\u001b[1;32m   1521\u001b[0m     eval_metric_ops = _verify_and_create_loss_metric(\n\u001b[1;32m   1522\u001b[0m         estimator_spec.eval_metric_ops, estimator_spec.loss)\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-182-039f518cf9fc>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m     46\u001b[0m         }\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0meval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-182-039f518cf9fc>\u001b[0m in \u001b[0;36mmetric_fn\u001b[0;34m(one_hot_labels, one_hot_predicted_labels)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Tuple of (value, update_op)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         precision = tf_metrics.precision(\n\u001b[0;32m---> 33\u001b[0;31m             one_hot_labels, one_hot_predicted_labels, num_labels, pos_indices, average=average)\n\u001b[0m\u001b[1;32m     34\u001b[0m         recall = tf_metrics.recall(\n\u001b[1;32m     35\u001b[0m             one_hot_labels, one_hot_predicted_labels, num_labels, pos_indices, average=average)\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/content-similarity-3.6.4/lib/python3.6/site-packages/tf_metrics/__init__.py\u001b[0m in \u001b[0;36mprecision\u001b[0;34m(labels, predictions, num_classes, pos_indices, weights, average)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[1;32m     41\u001b[0m     cm, op = _streaming_confusion_matrix(\n\u001b[0;32m---> 42\u001b[0;31m         labels, predictions, num_classes, weights)\n\u001b[0m\u001b[1;32m     43\u001b[0m     pr, _, _ = metrics_from_confusion_matrix(\n\u001b[1;32m     44\u001b[0m         cm, pos_indices, average=average)\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py\u001b[0m in \u001b[0;36m_streaming_confusion_matrix\u001b[0;34m(labels, predictions, num_classes, weights)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m   \u001b[0;31m# Flatten the input if its rank > 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "PPVEXhNjYXC-",
    "outputId": "dd5482cd-c558-465f-c854-ec11a0175316"
   },
   "outputs": [],
   "source": [
    "test_preds = estimator.predict(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/ellieking/Documents/content-similarity/BERT/bert_experiment_output/model.ckpt-1341\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "pred_list = list(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probabilities': array([-1.8557117, -1.5610144, -1.6814368, -1.6570656, -1.6684177,\n",
       "        -5.813551 , -7.3384275, -8.2665825, -8.073975 , -7.9055824,\n",
       "        -7.539958 , -7.947551 , -7.5277023, -7.6875324, -7.2341404,\n",
       "        -3.296257 , -6.9909315, -7.6236315, -8.402364 , -7.7072306,\n",
       "        -6.946128 , -7.2227106, -8.401003 , -7.058935 , -7.529442 ,\n",
       "        -8.596278 , -7.952729 , -7.92295  , -7.641367 , -7.811639 ,\n",
       "        -8.181353 , -7.3375435, -7.3382263, -8.124301 , -8.546824 ,\n",
       "        -7.9741344, -7.9750624, -7.7907763, -7.91045  , -7.8591003,\n",
       "        -8.067461 , -8.626943 , -7.9372845, -8.285039 , -8.051117 ,\n",
       "        -7.2252483, -8.675693 , -7.947386 , -7.9224377, -8.4383745,\n",
       "        -8.046499 , -8.909634 , -7.9269447, -8.507811 , -7.201808 ,\n",
       "        -8.185943 , -8.106163 , -8.790627 , -7.893656 , -7.669444 ,\n",
       "        -7.9067764, -7.7766495, -8.172624 , -7.9639587, -7.692419 ,\n",
       "        -8.278391 , -8.043953 , -8.630077 , -8.189078 , -8.236936 ,\n",
       "        -7.811632 , -8.580658 , -7.9502277, -8.507824 , -7.9998207,\n",
       "        -8.142132 , -7.953659 , -8.0074005, -8.0001335, -8.196322 ,\n",
       "        -8.276925 ], dtype=float32),\n",
       " 'labels': 1,\n",
       " 'y_true': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'y_pred': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([pred['y_true'] for pred in pred_list])\n",
    "y_pred = np.array([pred['y_pred'] for pred in pred_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7050, 81)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7050, 81)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ellieking/.pyenv/versions/3.6.4/envs/content-similarity-3.6.4/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.2139738 , 0.26589595, 0.20867769, 0.16358464, 0.16831683,\n",
       "        0.41263941, 0.58730159, 0.45283019, 0.5       , 0.5       ,\n",
       "        0.47826087, 0.56880734, 0.41153846, 0.6971831 , 0.62215909,\n",
       "        0.83870968, 0.41314554, 0.88888889, 1.        , 0.76785714,\n",
       "        0.78888889, 0.46218487, 0.        , 0.90353698, 0.56756757,\n",
       "        0.73333333, 0.83950617, 0.52884615, 0.91304348, 0.60606061,\n",
       "        0.87878788, 0.7       , 0.65957447, 0.68627451, 0.33898305,\n",
       "        0.3       , 0.        , 0.31147541, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.63888889, 0.        , 0.375     , 0.        , 1.        ,\n",
       "        0.84      , 0.83333333, 0.92307692, 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.83870968, 0.        , 0.        ,\n",
       "        0.        , 0.86153846, 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " array([0.24418605, 0.2296173 , 0.1697479 , 0.22072937, 0.15151515,\n",
       "        0.70253165, 0.83333333, 0.77419355, 0.81818182, 0.96666667,\n",
       "        0.33333333, 0.775     , 0.52195122, 0.73605948, 0.78214286,\n",
       "        0.26666667, 0.3876652 , 0.92753623, 0.07692308, 0.87755102,\n",
       "        0.93421053, 0.52380952, 0.        , 0.95904437, 0.24418605,\n",
       "        0.64705882, 0.7311828 , 0.80882353, 0.58333333, 0.97560976,\n",
       "        0.56862745, 0.71794872, 0.66428571, 0.97222222, 0.60606061,\n",
       "        0.14285714, 0.        , 0.80851064, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.52873563, 0.        , 0.22222222, 0.        , 0.75      ,\n",
       "        0.67741935, 0.8       , 0.94117647, 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.68421053, 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.17391304, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " array([0.22808379, 0.24642857, 0.18721038, 0.1879085 , 0.15947467,\n",
       "        0.51990632, 0.68901304, 0.57142857, 0.62068966, 0.65909091,\n",
       "        0.39285714, 0.65608466, 0.46021505, 0.71609403, 0.69303797,\n",
       "        0.40466926, 0.4       , 0.90780142, 0.14285714, 0.81904762,\n",
       "        0.85542169, 0.49107143, 0.        , 0.93046358, 0.34146341,\n",
       "        0.6875    , 0.7816092 , 0.63953488, 0.71186441, 0.74766355,\n",
       "        0.69047619, 0.70886076, 0.66192171, 0.8045977 , 0.43478261,\n",
       "        0.19354839, 0.        , 0.44970414, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.57861635, 0.        , 0.27906977, 0.        , 0.85714286,\n",
       "        0.75      , 0.81632653, 0.93203883, 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.75362319, 0.        , 0.        ,\n",
       "        0.        , 0.92561983, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.2962963 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " array([602, 601, 595, 521, 561, 158, 222,  31,  44,  30,  66,  80, 205,\n",
       "        269, 280, 195, 227,  69,  26,  98, 152, 105,  17, 293, 172,  17,\n",
       "         93,  68,  36,  41,  51,  78, 140,  72,  33,  21,  20,  47,  15,\n",
       "          5,  24,  11,   5,   1,  12,  87,   2,  27,  20,  12,  31,  50,\n",
       "         51,  15,  24,  19,  11,  38,  25,   4,   5,  56,   7,  13,  10,\n",
       "          3,  23,  14,   9,  25,   6,   1,   1,   9,   9,  12,   3,  16,\n",
       "          1,   1,   1]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_true, y_pred, beta=1.0,  pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(next(test_preds)['probabilities'], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ueKsULteiz1B"
   },
   "source": [
    "Now let's write code to make predictions on new sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsrbTD2EJTVl"
   },
   "outputs": [],
   "source": [
    "def getPrediction(in_sentences):\n",
    "  labels = [\"Negative\", \"Positive\"]\n",
    "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "  predictions = estimator.predict(predict_input_fn)\n",
    "  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-thbodgih_VJ"
   },
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The acting was a bit lacking\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "QrZmvZySKQTm",
    "outputId": "3891fafb-a460-4eb8-fa6c-335a5bbc10e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 4\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: \n",
      "INFO:tensorflow:tokens: [CLS] that movie was absolutely awful [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2008 3185 2001 7078 9643 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: \n",
      "INFO:tensorflow:tokens: [CLS] the acting was a bit lacking [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1996 3772 2001 1037 2978 11158 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: \n",
      "INFO:tensorflow:tokens: [CLS] the film was creative and surprising [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1996 2143 2001 5541 1998 11341 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: \n",
      "INFO:tensorflow:tokens: [CLS] absolutely fantastic ! [SEP]\n",
      "INFO:tensorflow:input_ids: 101 7078 10392 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://bert-tfhub/aclImdb_v1/model.ckpt-468\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predictions = getPrediction(pred_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXkRiEBUqN3n"
   },
   "source": [
    "Voila! We have a sentiment classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "ERkTE8-7oQLZ",
    "outputId": "26c33224-dc2c-4b3d-f7b4-ac3ef0a58b27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('That movie was absolutely awful',\n",
       "  array([-4.9142293e-03, -5.3180690e+00], dtype=float32),\n",
       "  'Negative'),\n",
       " ('The acting was a bit lacking',\n",
       "  array([-0.03325794, -3.4200459 ], dtype=float32),\n",
       "  'Negative'),\n",
       " ('The film was creative and surprising',\n",
       "  array([-5.3589125e+00, -4.7171740e-03], dtype=float32),\n",
       "  'Positive'),\n",
       " ('Absolutely fantastic!',\n",
       "  array([-5.0434084 , -0.00647258], dtype=float32),\n",
       "  'Positive')]"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_business_tagging_deeper.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb",
     "timestamp": 1551188138792
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
