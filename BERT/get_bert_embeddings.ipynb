{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in labelled\n"
     ]
    }
   ],
   "source": [
    "# ### Set-up and get data\n",
    "\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "DATADIR = os.getenv(\"DATADIR\")\n",
    "\n",
    "print(\"read in labelled\")\n",
    "labelled = pd.read_csv(os.path.join(DATADIR, 'labelled.csv.gz'), compression='gzip', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(labelled, test_size=0.33, random_state=42, stratify=labelled['level1taxon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204821, 19)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100882, 19)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'combined_text'\n",
    "LABEL_COLUMN = 'level1taxon'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "\n",
    "label_list = list(labelled.level1taxon.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(module_url)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 204821\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] di ##t round ##table boost ##s uk business opportunities in pakistan international trade minister greg hands hosted a trade round ##table to help uk business secure contracts for chinese investment in pakistan . china is supporting $ 51 billion of infrastructure development in pakistan as part of the ‘ china pakistan economic corridor ’ ( cp ##ec ) to develop key infrastructure projects like roads railways and power stations which will modern ##ise pakistan ’ s economy and boost access to trade . it is part of china ’ s broader ‘ belt and road initiative ’ to replicate the ancient silk road trade routes with modern trading relationships and investments across asia the middle east and into europe . greg hands met yesterday ( 4 april ) with leading uk businesses policy experts and senior representatives of the chinese and pakistani governments as the uk is poised to be a key partner of cp ##ec ahead of a larger cp ##ec conference in islamabad in may being hosted by the uk . international trade minister greg hands said : britain is a country of free trade influence and can be an important partner for china and pakistan in the delivery of huge infrastructure projects that are being planned between the 2 countries . as part of an outward looking global britain we have a clear ambition to increase trade with both china and pakistan and uk businesses are well placed to capital ##ise on the new opportunities the region . participants in the round ##table included the chinese ambassador pakistani high commissioner to the uk and uk high commissioner to pakistan . as well as experts from city ##uk the royal united services institute and the china britain business council . businesses including hs ##bc del ##oit ##te and standard chartered also discussed how they and other british firms can support the delivery of cp ##ec . a joint statement in 2015 between the uk and chinese governments committed both countries to support each other ’ s commercial co operation in new markets including the belt and road and the $ 51 billion being invested by china in pakistan presents big opportunities for uk businesses in the next few years . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 4487 2102 2461 10880 12992 2015 2866 2449 6695 1999 4501 2248 3119 2704 6754 2398 4354 1037 3119 2461 10880 2000 2393 2866 2449 5851 8311 2005 2822 5211 1999 4501 1012 2859 2003 4637 1002 4868 4551 1997 6502 2458 1999 4501 2004 2112 1997 1996 1520 2859 4501 3171 7120 1521 1006 18133 8586 1007 2000 4503 3145 6502 3934 2066 4925 7111 1998 2373 3703 2029 2097 2715 5562 4501 1521 1055 4610 1998 12992 3229 2000 3119 1012 2009 2003 2112 1997 2859 1521 1055 12368 1520 5583 1998 2346 6349 1521 2000 28024 1996 3418 6953 2346 3119 5847 2007 2715 6202 6550 1998 10518 2408 4021 1996 2690 2264 1998 2046 2885 1012 6754 2398 2777 7483 1006 1018 2258 1007 2007 2877 2866 5661 3343 8519 1998 3026 4505 1997 1996 2822 1998 9889 6867 2004 1996 2866 2003 22303 2000 2022 1037 3145 4256 1997 18133 8586 3805 1997 1037 3469 18133 8586 3034 1999 26905 1999 2089 2108 4354 2011 1996 2866 1012 2248 3119 2704 6754 2398 2056 1024 3725 2003 1037 2406 1997 2489 3119 3747 1998 2064 2022 2019 2590 4256 2005 2859 1998 4501 1999 1996 6959 1997 4121 6502 3934 2008 2024 2108 3740 2090 1996 1016 3032 1012 2004 2112 1997 2019 15436 2559 3795 3725 2057 2031 1037 3154 16290 2000 3623 3119 2007 2119 2859 1998 4501 1998 2866 5661 2024 2092 2872 2000 3007 5562 2006 1996 2047 6695 1996 2555 1012 6818 1999 1996 2461 10880 2443 1996 2822 6059 9889 2152 5849 2000 1996 2866 1998 2866 2152 5849 2000 4501 1012 2004 2092 2004 8519 2013 2103 6968 1996 2548 2142 2578 2820 1998 1996 2859 3725 2449 2473 1012 5661 2164 26236 9818 3972 28100 2618 1998 3115 12443 2036 6936 2129 2027 1998 2060 2329 9786 2064 2490 1996 6959 1997 18133 8586 1012 1037 4101 4861 1999 2325 2090 1996 2866 1998 2822 6867 5462 2119 3032 2000 2490 2169 2060 1521 1055 3293 2522 3169 1999 2047 6089 2164 1996 5583 1998 2346 1998 1996 1002 4868 4551 2108 11241 2011 2859 1999 4501 7534 2502 6695 2005 2866 5661 1999 1996 2279 2261 2086 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: Business and industry (id = 10)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] provisional local government finance settlement 2019 to 2020 : consultation consultation on the local government financial settlement for 2019 to 2020 . this consultation seeks views on proposals for the local government finance settlement for 2019 to 2020 . provisional local government finance settlement 2019 to 2020 : consultation ref : isbn 978 - 1 - 40 ##9 ##8 - 53 ##8 ##8 - 6 pdf 35 ##3 ##k ##b 27 pages this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email alternative ##form ##ats @ communities . gov . uk . please tell us what format you need . it will help us if you say what assist ##ive technology you use . this document provides a summary of responses received to the consultation on the provisional local government finance settlement . provisional local government finance settlement 2019 to 2020 : summary of responses ref : isbn 978 - 1 - 40 ##9 ##8 - 54 ##22 - 7 pdf 257 ##k ##b 13 pages this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email alternative ##form ##ats @ communities . gov . uk . please tell us what format you need . it will help us if you say what assist ##ive technology you use . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 10864 2334 2231 5446 4093 10476 2000 12609 1024 16053 16053 2006 1996 2334 2231 3361 4093 2005 10476 2000 12609 1012 2023 16053 11014 5328 2006 10340 2005 1996 2334 2231 5446 4093 2005 10476 2000 12609 1012 10864 2334 2231 5446 4093 10476 2000 12609 1024 16053 25416 1024 3175 4891 1011 1015 1011 2871 2683 2620 1011 5187 2620 2620 1011 1020 11135 3486 2509 2243 2497 2676 5530 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 4522 14192 11149 1030 4279 1012 18079 1012 2866 1012 3531 2425 2149 2054 4289 2017 2342 1012 2009 2097 2393 2149 2065 2017 2360 2054 6509 3512 2974 2017 2224 1012 2023 6254 3640 1037 12654 1997 10960 2363 2000 1996 16053 2006 1996 10864 2334 2231 5446 4093 1012 10864 2334 2231 5446 4093 10476 2000 12609 1024 12654 1997 10960 25416 1024 3175 4891 1011 1015 1011 2871 2683 2620 1011 5139 19317 1011 1021 11135 24368 2243 2497 2410 5530 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 4522 14192 11149 1030 4279 1012 18079 1012 2866 1012 3531 2425 2149 2054 4289 2017 2342 1012 2009 2097 2393 2149 2065 2017 2360 2054 6509 3512 2974 2017 2224 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: Regional and local government (id = 14)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] west midlands rail franchise competition 2015 : expression of interest documentation pre - qualification information issued in 2015 for potential bid ##ders in the west midlands franchise competition . this pre qualification information is for potential bid ##ders in the west midlands franchise competition and includes the following : ‘ expression of interest ’ ( e ##oi ) document and appendix spreads ##hee ##t ‘ further technical questions ’ ‘ pre qualification process document ’ ( pp ##d ) ‘ franchise letting process agreement ’ ( fl ##pa ) more about the west midlands rail franchise . expression of interest ( e ##oi ) document pdf 163 ##k ##b 29 pages this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email web ##master ##df ##t @ d ##ft . gov . uk . please tell us what format you need . it will help us if you say what assist ##ive technology you use . economic and financial standing spreads ##hee ##t ms excel spreads ##hee ##t 157 ##k ##b this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email web ##master ##df ##t @ d ##ft . gov . uk . please tell us what format you need . it will help us if you say what assist ##ive technology you use . further technical questions pdf 57 . 1 ##k ##b 6 pages this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email web ##master ##df ##t @ d ##ft . gov . uk . please tell us what format you need . it will help us if you say what assist ##ive technology you use . pre - qualification process document pdf 53 ##7 ##k ##b 43 pages this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email web ##master ##df ##t @ d ##ft . gov . uk . please tell us what format you need . it will help us if you say what assist ##ive technology you use . franchise letting process agreement pdf 47 ##8 ##k ##b 26 pages this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2225 13256 4334 6329 2971 2325 1024 3670 1997 3037 12653 3653 1011 8263 2592 3843 1999 2325 2005 4022 7226 13375 1999 1996 2225 13256 6329 2971 1012 2023 3653 8263 2592 2003 2005 4022 7226 13375 1999 1996 2225 13256 6329 2971 1998 2950 1996 2206 1024 1520 3670 1997 3037 1521 1006 1041 10448 1007 6254 1998 22524 20861 21030 2102 1520 2582 4087 3980 1521 1520 3653 8263 2832 6254 1521 1006 4903 2094 1007 1520 6329 5599 2832 3820 1521 1006 13109 4502 1007 2062 2055 1996 2225 13256 4334 6329 1012 3670 1997 3037 1006 1041 10448 1007 6254 11135 17867 2243 2497 2756 5530 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 4773 8706 20952 2102 1030 1040 6199 1012 18079 1012 2866 1012 3531 2425 2149 2054 4289 2017 2342 1012 2009 2097 2393 2149 2065 2017 2360 2054 6509 3512 2974 2017 2224 1012 3171 1998 3361 3061 20861 21030 2102 5796 24970 20861 21030 2102 17403 2243 2497 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 4773 8706 20952 2102 1030 1040 6199 1012 18079 1012 2866 1012 3531 2425 2149 2054 4289 2017 2342 1012 2009 2097 2393 2149 2065 2017 2360 2054 6509 3512 2974 2017 2224 1012 2582 4087 3980 11135 5401 1012 1015 2243 2497 1020 5530 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 4773 8706 20952 2102 1030 1040 6199 1012 18079 1012 2866 1012 3531 2425 2149 2054 4289 2017 2342 1012 2009 2097 2393 2149 2065 2017 2360 2054 6509 3512 2974 2017 2224 1012 3653 1011 8263 2832 6254 11135 5187 2581 2243 2497 4724 5530 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 4773 8706 20952 2102 1030 1040 6199 1012 18079 1012 2866 1012 3531 2425 2149 2054 4289 2017 2342 1012 2009 2097 2393 2149 2065 2017 2360 2054 6509 3512 2974 2017 2224 1012 6329 5599 2832 3820 11135 4700 2620 2243 2497 2656 5530 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: Business and industry (id = 10)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] index of production : march 2015 measures the volume of production at base year prices for the manufacturing mining and quarry ##ing and energy supply industries . these are seasonal ##ly adjusted figures on the index of output of the production industries . official statistics are produced imp ##art ##ial ##ly and free from any political influence . index of production : march 2015 http : / / www . on ##s . gov . uk / on ##s / re ##l / io ##p / index - of - production / index . html [SEP]\n",
      "INFO:tensorflow:input_ids: 101 5950 1997 2537 1024 2233 2325 5761 1996 3872 1997 2537 2012 2918 2095 7597 2005 1996 5814 5471 1998 12907 2075 1998 2943 4425 6088 1012 2122 2024 12348 2135 10426 4481 2006 1996 5950 1997 6434 1997 1996 2537 6088 1012 2880 6747 2024 2550 17727 8445 4818 2135 1998 2489 2013 2151 2576 3747 1012 5950 1997 2537 1024 2233 2325 8299 1024 1013 1013 7479 1012 2006 2015 1012 18079 1012 2866 1013 2006 2015 1013 2128 2140 1013 22834 2361 1013 5950 1011 1997 1011 2537 1013 5950 1012 16129 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: Business and industry (id = 10)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] statement of practice 2 ( 1996 ) poole ##d cars - incident ##al private use . statements applicable to individuals ( income tax and interest on tax ) . statement of practice 2 ( 1996 ) html [SEP]\n",
      "INFO:tensorflow:input_ids: 101 4861 1997 3218 1016 1006 2727 1007 19107 2094 3765 1011 5043 2389 2797 2224 1012 8635 12711 2000 3633 1006 3318 4171 1998 3037 2006 4171 1007 1012 4861 1997 3218 1016 1006 2727 1007 16129 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: Money (id = 12)\n",
      "INFO:tensorflow:Writing example 10000 of 204821\n",
      "INFO:tensorflow:Writing example 20000 of 204821\n",
      "INFO:tensorflow:Writing example 30000 of 204821\n",
      "INFO:tensorflow:Writing example 40000 of 204821\n",
      "INFO:tensorflow:Writing example 50000 of 204821\n",
      "INFO:tensorflow:Writing example 60000 of 204821\n",
      "INFO:tensorflow:Writing example 70000 of 204821\n",
      "INFO:tensorflow:Writing example 80000 of 204821\n",
      "INFO:tensorflow:Writing example 90000 of 204821\n",
      "INFO:tensorflow:Writing example 100000 of 204821\n",
      "INFO:tensorflow:Writing example 110000 of 204821\n",
      "INFO:tensorflow:Writing example 120000 of 204821\n",
      "INFO:tensorflow:Writing example 130000 of 204821\n",
      "INFO:tensorflow:Writing example 140000 of 204821\n",
      "INFO:tensorflow:Writing example 150000 of 204821\n",
      "INFO:tensorflow:Writing example 160000 of 204821\n",
      "INFO:tensorflow:Writing example 170000 of 204821\n",
      "INFO:tensorflow:Writing example 180000 of 204821\n",
      "INFO:tensorflow:Writing example 190000 of 204821\n",
      "INFO:tensorflow:Writing example 200000 of 204821\n",
      "INFO:tensorflow:Writing example 0 of 100882\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] outer thames estuary special protection area extension : consultation outcome natural england and the joint nature conservation committee are seeking views on the proposal to extend the special protection area ( spa ) marine site . this consultation has been extended by 12 weeks until 5 ##pm on 14 july 2016 to allow some stakeholders who were previously not informed of it to respond to the proposals . all previous representations received will be considered as valid responses and submitted to government . special protection areas ( spa ##s ) are special sites designated under the eu birds directive to protect rare vulnerable and migratory birds . the proposal for the outer thames estuary is to extend the boundary of the existing spa in several places to protect the internationally important colony of : little ter ##n common ter ##n the extended areas include waters adjacent to the existing breeding colonies which are the ideal feeding habitat for these birds . the proposal is available as a series of separate documents ( attached above ) . you should : read the consultation summary that sets out the aims of the proposal read the departmental brief that provides the scientific explanation behind the proposal use the maps to locate the proposal site ( s ) use the ‘ respond online ’ link below to give your views . outer thames estuary spa : consultation summary pdf 43 ##2 ##k ##b 5 pages this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email en ##qui ##ries @ natural ##eng ##land . org . uk . please tell us what format you need . it will help us if you say what assist ##ive technology you use . outer thames estuary spa : departmental brief pdf 1 . 99 ##mb 72 pages this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email en ##qui ##ries @ natural ##eng ##land . org . uk . please tell us what format you need . it will help us if you say what assist ##ive technology you use . outer thames estuary spa : boundary map pdf 3 . 68 ##mb 1 page this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email en ##qui ##ries @ natural ##eng ##land . org . uk . please tell us what format you need . it will help us if you say what assist ##ive technology you use . outer thames estuary spa [SEP]\n",
      "INFO:tensorflow:input_ids: 101 6058 11076 18056 2569 3860 2181 5331 1024 16053 9560 3019 2563 1998 1996 4101 3267 5680 2837 2024 6224 5328 2006 1996 6378 2000 7949 1996 2569 3860 2181 1006 12403 1007 3884 2609 1012 2023 16053 2038 2042 3668 2011 2260 3134 2127 1019 9737 2006 2403 2251 2355 2000 3499 2070 22859 2040 2020 3130 2025 6727 1997 2009 2000 6869 2000 1996 10340 1012 2035 3025 15066 2363 2097 2022 2641 2004 9398 10960 1998 7864 2000 2231 1012 2569 3860 2752 1006 12403 2015 1007 2024 2569 4573 4351 2104 1996 7327 5055 16449 2000 4047 4678 8211 1998 22262 5055 1012 1996 6378 2005 1996 6058 11076 18056 2003 2000 7949 1996 6192 1997 1996 4493 12403 1999 2195 3182 2000 4047 1996 7587 2590 5701 1997 1024 2210 28774 2078 2691 28774 2078 1996 3668 2752 2421 5380 5516 2000 1996 4493 8119 8355 2029 2024 1996 7812 8521 6552 2005 2122 5055 1012 1996 6378 2003 2800 2004 1037 2186 1997 3584 5491 1006 4987 2682 1007 1012 2017 2323 1024 3191 1996 16053 12654 2008 4520 2041 1996 8704 1997 1996 6378 3191 1996 28912 4766 2008 3640 1996 4045 7526 2369 1996 6378 2224 1996 7341 2000 12453 1996 6378 2609 1006 1055 1007 2224 1996 1520 6869 3784 1521 4957 2917 2000 2507 2115 5328 1012 6058 11076 18056 12403 1024 16053 12654 11135 4724 2475 2243 2497 1019 5530 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 4372 15549 5134 1030 3019 13159 3122 1012 8917 1012 2866 1012 3531 2425 2149 2054 4289 2017 2342 1012 2009 2097 2393 2149 2065 2017 2360 2054 6509 3512 2974 2017 2224 1012 6058 11076 18056 12403 1024 28912 4766 11135 1015 1012 5585 14905 5824 5530 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 4372 15549 5134 1030 3019 13159 3122 1012 8917 1012 2866 1012 3531 2425 2149 2054 4289 2017 2342 1012 2009 2097 2393 2149 2065 2017 2360 2054 6509 3512 2974 2017 2224 1012 6058 11076 18056 12403 1024 6192 4949 11135 1017 1012 6273 14905 1015 3931 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 4372 15549 5134 1030 3019 13159 3122 1012 8917 1012 2866 1012 3531 2425 2149 2054 4289 2017 2342 1012 2009 2097 2393 2149 2065 2017 2360 2054 6509 3512 2974 2017 2224 1012 6058 11076 18056 12403 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: Environment (id = 3)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] small business tax review terms of reference for the small business tax review which identifies areas of the tax system that cause complexity and and recommends sim ##pl ##ification measures . press release read the press release on recommendations to sim ##plify the uk ’ s tax system for small businesses government response the government has published a number of documents as part of the response to our review of small business tax . making tax easier quicker and simpler for small business response to the office of tax sim ##pl ##ification ’ s small business tax review final report : hm ##rc administration simpler income tax for the simplest small businesses : consultation document consultation on a di ##sin ##corp ##oration relief hm ##rc ir ##35 guidance small business tax review : terms of reference html small business tax review final report : hm ##rc administration pdf 53 ##4 ##k ##b 54 pages this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email digital . communications @ hm ##tre ##as ##ury . gs ##i . gov . uk . please tell us what format you need . it will help us if you say what assist ##ive technology you use . small business tax review final report : simpler income tax for the smallest businesses pdf 48 ##6 ##k ##b 57 pages this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email digital . communications @ hm ##tre ##as ##ury . gs ##i . gov . uk . please tell us what format you need . it will help us if you say what assist ##ive technology you use . small business tax review final report : di ##sin ##corp ##oration relief pdf 41 ##8 ##k ##b 39 pages this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email digital . communications @ hm ##tre ##as ##ury . gs ##i . gov . uk . please tell us what format you need . it will help us if you say what assist ##ive technology you use . hm ##rc summary report : understanding small businesses ’ experience of the tax system pdf 109 ##k ##b 11 pages this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email digital . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2235 2449 4171 3319 3408 1997 4431 2005 1996 2235 2449 4171 3319 2029 14847 2752 1997 1996 4171 2291 2008 3426 11619 1998 1998 26021 21934 24759 9031 5761 1012 2811 2713 3191 1996 2811 2713 2006 11433 2000 21934 28250 1996 2866 1521 1055 4171 2291 2005 2235 5661 2231 3433 1996 2231 2038 2405 1037 2193 1997 5491 2004 2112 1997 1996 3433 2000 2256 3319 1997 2235 2449 4171 1012 2437 4171 6082 19059 1998 16325 2005 2235 2449 3433 2000 1996 2436 1997 4171 21934 24759 9031 1521 1055 2235 2449 4171 3319 2345 3189 1024 20287 11890 3447 16325 3318 4171 2005 1996 21304 2235 5661 1024 16053 6254 16053 2006 1037 4487 11493 24586 21223 4335 20287 11890 20868 19481 8606 2235 2449 4171 3319 1024 3408 1997 4431 16129 2235 2449 4171 3319 2345 3189 1024 20287 11890 3447 11135 5187 2549 2243 2497 5139 5530 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 3617 1012 4806 1030 20287 7913 3022 13098 1012 28177 2072 1012 18079 1012 2866 1012 3531 2425 2149 2054 4289 2017 2342 1012 2009 2097 2393 2149 2065 2017 2360 2054 6509 3512 2974 2017 2224 1012 2235 2449 4171 3319 2345 3189 1024 16325 3318 4171 2005 1996 10479 5661 11135 4466 2575 2243 2497 5401 5530 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 3617 1012 4806 1030 20287 7913 3022 13098 1012 28177 2072 1012 18079 1012 2866 1012 3531 2425 2149 2054 4289 2017 2342 1012 2009 2097 2393 2149 2065 2017 2360 2054 6509 3512 2974 2017 2224 1012 2235 2449 4171 3319 2345 3189 1024 4487 11493 24586 21223 4335 11135 4601 2620 2243 2497 4464 5530 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 3617 1012 4806 1030 20287 7913 3022 13098 1012 28177 2072 1012 18079 1012 2866 1012 3531 2425 2149 2054 4289 2017 2342 1012 2009 2097 2393 2149 2065 2017 2360 2054 6509 3512 2974 2017 2224 1012 20287 11890 12654 3189 1024 4824 2235 5661 1521 3325 1997 1996 4171 2291 11135 11518 2243 2497 2340 5530 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 3617 1012 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: Money (id = 12)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] opportunity for charities to become authorised advisors to eu citizens applying to settle in the uk a new scheme for charities to become registered advisors to eu citizens when they apply to settle in the uk . the office of the immigration services commissioner ( o ##is ##c ) has launched a new scheme for charities and other organisations to apply more quickly to become registered advisors to eu citizens when they apply to settle in the uk . the scheme is aimed at smaller community based charities and other organisations including faith based and local advice groups . successful applicants will be able to provide basic ‘ level 1 ’ advice ( solely on eu settlement ) without needing to complete the written exam which is usually part of the application process . the streamlined process will allow applications to be considered more quickly with decisions expected 4 to 6 weeks after receipt . it is a criminal offence for a person to provide immigration advice or services in the uk unless their organisation is regulated by the o ##is ##c or meets certain other specific criteria . details about what assistance can be given to migrants without the requirement to be regulated can be found in the o ##is ##c immigration assistance document . further guidance for the community and voluntary sector about registering with the o ##is ##c as well as information on the new eu settlement scheme registration is available on the o ##is ##c website . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 4495 2005 15430 2000 2468 19256 18934 2000 7327 4480 11243 2000 7392 1999 1996 2866 1037 2047 5679 2005 15430 2000 2468 5068 18934 2000 7327 4480 2043 2027 6611 2000 7392 1999 1996 2866 1012 1996 2436 1997 1996 7521 2578 5849 1006 1051 2483 2278 1007 2038 3390 1037 2047 5679 2005 15430 1998 2060 8593 2000 6611 2062 2855 2000 2468 5068 18934 2000 7327 4480 2043 2027 6611 2000 7392 1999 1996 2866 1012 1996 5679 2003 6461 2012 3760 2451 2241 15430 1998 2060 8593 2164 4752 2241 1998 2334 6040 2967 1012 3144 17362 2097 2022 2583 2000 3073 3937 1520 2504 1015 1521 6040 1006 9578 2006 7327 4093 1007 2302 11303 2000 3143 1996 2517 11360 2029 2003 2788 2112 1997 1996 4646 2832 1012 1996 29445 2832 2097 3499 5097 2000 2022 2641 2062 2855 2007 6567 3517 1018 2000 1020 3134 2044 24306 1012 2009 2003 1037 4735 15226 2005 1037 2711 2000 3073 7521 6040 2030 2578 1999 1996 2866 4983 2037 5502 2003 12222 2011 1996 1051 2483 2278 2030 6010 3056 2060 3563 9181 1012 4751 2055 2054 5375 2064 2022 2445 2000 16836 2302 1996 9095 2000 2022 12222 2064 2022 2179 1999 1996 1051 2483 2278 7521 5375 6254 1012 2582 8606 2005 1996 2451 1998 10758 4753 2055 25719 2007 1996 1051 2483 2278 2004 2092 2004 2592 2006 1996 2047 7327 4093 5679 8819 2003 2800 2006 1996 1051 2483 2278 4037 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: Society and culture (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] intercity west coast details of the award process the aims and objectives for the intercity west coast franchise and the proposed specification . the intercity west coast franchise is expected to commence on 1 april 2012 . this document aims to inform stakeholders of the : award process aims and objectives for the franchise proposed base franchise specification . intercity west coast consultation pdf 630 ##k ##b this file may not be suitable for users of assist ##ive technology . request an accessible format . if you use assist ##ive technology ( such as a screen reader ) and need a version of this document in a more accessible format please email web ##master ##df ##t @ d ##ft . gov . uk . please tell us what format you need . it will help us if you say what assist ##ive technology you use . this consultation was superseded by the new rail fran ##chi ##sing programme that commenced in 2013 . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 20651 2225 3023 4751 1997 1996 2400 2832 1996 8704 1998 11100 2005 1996 20651 2225 3023 6329 1998 1996 3818 12827 1012 1996 20651 2225 3023 6329 2003 3517 2000 22825 2006 1015 2258 2262 1012 2023 6254 8704 2000 12367 22859 1997 1996 1024 2400 2832 8704 1998 11100 2005 1996 6329 3818 2918 6329 12827 1012 20651 2225 3023 16053 11135 23609 2243 2497 2023 5371 2089 2025 2022 7218 2005 5198 1997 6509 3512 2974 1012 5227 2019 7801 4289 1012 2065 2017 2224 6509 3512 2974 1006 2107 2004 1037 3898 8068 1007 1998 2342 1037 2544 1997 2023 6254 1999 1037 2062 7801 4289 3531 10373 4773 8706 20952 2102 1030 1040 6199 1012 18079 1012 2866 1012 3531 2425 2149 2054 4289 2017 2342 1012 2009 2097 2393 2149 2065 2017 2360 2054 6509 3512 2974 2017 2224 1012 2023 16053 2001 19886 2011 1996 2047 4334 23151 5428 7741 4746 2008 8420 1999 2286 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: Business and industry (id = 10)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] mr q pa v st mu ##ngo community housing association : 320 ##11 ##9 ##9 / 2018 employment tribunal decision . read the full decision in mr q pa v st mu ##ngo community housing association : : 320 ##11 ##9 ##9 / 2018 judgment . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2720 1053 6643 1058 2358 14163 16656 2451 3847 2523 1024 13710 14526 2683 2683 1013 2760 6107 12152 3247 1012 3191 1996 2440 3247 1999 2720 1053 6643 1058 2358 14163 16656 2451 3847 2523 1024 1024 13710 14526 2683 2683 1013 2760 8689 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: Crime, justice and law (id = 5)\n",
      "INFO:tensorflow:Writing example 10000 of 100882\n",
      "INFO:tensorflow:Writing example 20000 of 100882\n",
      "INFO:tensorflow:Writing example 30000 of 100882\n",
      "INFO:tensorflow:Writing example 40000 of 100882\n",
      "INFO:tensorflow:Writing example 50000 of 100882\n",
      "INFO:tensorflow:Writing example 60000 of 100882\n",
      "INFO:tensorflow:Writing example 70000 of 100882\n",
      "INFO:tensorflow:Writing example 80000 of 100882\n",
      "INFO:tensorflow:Writing example 90000 of 100882\n",
      "INFO:tensorflow:Writing example 100000 of 100882\n"
     ]
    }
   ],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 512\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Prepare data\n",
    "corpus = clean_content['combined_text'].tolist()\n",
    "\n",
    "TEXT_LENGTH = 512\n",
    "short_corpus=[]\n",
    "for text in corpus:\n",
    "    words = text.split()\n",
    "    truncated = \" \".join(words[0:TEXT_LENGTH])\n",
    "    short_corpus.append(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "    bert_module = hub.Module(module_url, trainable=True)\n",
    "    bert_inputs = dict(\n",
    "        input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "        inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for politeness data.\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "\n",
    "        # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "        predicted_labels = tf.squeeze(\n",
    "            tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "\n",
    "        # If we're train/eval, compute loss between predicted and actual label\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "\n",
    "        # TRAIN and EVAL\n",
    "        if not is_predicting:\n",
    "\n",
    "            (loss, predicted_labels, log_probs) = create_model(\n",
    "                is_predicting, input_ids, input_mask, segment_ids, label_ids,\n",
    "                num_labels)\n",
    "\n",
    "            train_op = bert.optimization.create_optimizer(\n",
    "                loss,\n",
    "                learning_rate,\n",
    "                num_train_steps,\n",
    "                num_warmup_steps,\n",
    "                use_tpu=False)\n",
    "\n",
    "            # Calculate evaluation metrics.\n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                f1_score = tf.contrib.metrics.f1_score(label_ids,\n",
    "                                                       predicted_labels)\n",
    "                auc = tf.metrics.auc(label_ids, predicted_labels)\n",
    "                recall = tf.metrics.recall(label_ids, predicted_labels)\n",
    "                precision = tf.metrics.precision(label_ids, predicted_labels)\n",
    "                true_pos = tf.metrics.true_positives(label_ids,\n",
    "                                                     predicted_labels)\n",
    "                true_neg = tf.metrics.true_negatives(label_ids,\n",
    "                                                     predicted_labels)\n",
    "                false_pos = tf.metrics.false_positives(label_ids,\n",
    "                                                       predicted_labels)\n",
    "                false_neg = tf.metrics.false_negatives(label_ids,\n",
    "                                                       predicted_labels)\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"f1_score\": f1_score,\n",
    "                    \"auc\": auc,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"true_positives\": true_pos,\n",
    "                    \"true_negatives\": true_neg,\n",
    "                    \"false_positives\": false_pos,\n",
    "                    \"false_negatives\": false_neg\n",
    "                }\n",
    "\n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(\n",
    "                    mode=mode, loss=loss, train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(\n",
    "                    mode=mode, loss=loss, eval_metric_ops=eval_metrics)\n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(\n",
    "                is_predicting, input_ids, input_mask, segment_ids, label_ids,\n",
    "                num_labels)\n",
    "\n",
    "            predictions = {\n",
    "                'probabilities': log_probs,\n",
    "                'labels': predicted_labels\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Return the actual model function in the closure\n",
    "    return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=DATADIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/ellieking/Documents/govuk-taxonomy-supervised-learning/data/2019-02-11', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1f6c797f0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/ellieking/Documents/govuk-taxonomy-supervised-learning/data/2019-02-11/model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /Users/ellieking/Documents/govuk-taxonomy-supervised-learning/data/2019-02-11/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.12635, step = 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-6187edf75f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Beginning Training!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcurrent_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_train_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training took time \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1205\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1239\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1240\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1469\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    669\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1157\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1238\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/content-similarity-3.6.4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ellieking/Documents/content-similarity/BERT\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step (DT_INT64) []\n",
      "module/bert/embeddings/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/embeddings/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/embeddings/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/embeddings/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/embeddings/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/embeddings/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/embeddings/position_embeddings (DT_FLOAT) [512,768]\n",
      "module/bert/embeddings/position_embeddings/adam_m (DT_FLOAT) [512,768]\n",
      "module/bert/embeddings/position_embeddings/adam_v (DT_FLOAT) [512,768]\n",
      "module/bert/embeddings/token_type_embeddings (DT_FLOAT) [2,768]\n",
      "module/bert/embeddings/token_type_embeddings/adam_m (DT_FLOAT) [2,768]\n",
      "module/bert/embeddings/token_type_embeddings/adam_v (DT_FLOAT) [2,768]\n",
      "module/bert/embeddings/word_embeddings (DT_FLOAT) [30522,768]\n",
      "module/bert/embeddings/word_embeddings/adam_m (DT_FLOAT) [30522,768]\n",
      "module/bert/embeddings/word_embeddings/adam_v (DT_FLOAT) [30522,768]\n",
      "module/bert/encoder/layer_0/attention/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/output/dense/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_0/attention/output/dense/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_0/attention/output/dense/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_0/attention/self/key/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/self/key/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/self/key/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/self/key/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_0/attention/self/key/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_0/attention/self/key/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_0/attention/self/query/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/self/query/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/self/query/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/self/query/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_0/attention/self/query/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_0/attention/self/query/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_0/attention/self/value/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/self/value/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/self/value/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/attention/self/value/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_0/attention/self/value/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_0/attention/self/value/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_0/intermediate/dense/bias (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_0/intermediate/dense/bias/adam_m (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_0/intermediate/dense/bias/adam_v (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_0/intermediate/dense/kernel (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_0/intermediate/dense/kernel/adam_m (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_0/intermediate/dense/kernel/adam_v (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_0/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_0/output/dense/kernel (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_0/output/dense/kernel/adam_m (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_0/output/dense/kernel/adam_v (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_1/attention/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/output/dense/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_1/attention/output/dense/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_1/attention/output/dense/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_1/attention/self/key/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/self/key/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/self/key/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/self/key/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_1/attention/self/key/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_1/attention/self/key/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_1/attention/self/query/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/self/query/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/self/query/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/self/query/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_1/attention/self/query/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_1/attention/self/query/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_1/attention/self/value/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/self/value/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/self/value/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/attention/self/value/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_1/attention/self/value/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_1/attention/self/value/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_1/intermediate/dense/bias (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_1/intermediate/dense/bias/adam_m (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_1/intermediate/dense/bias/adam_v (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_1/intermediate/dense/kernel (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_1/intermediate/dense/kernel/adam_m (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_1/intermediate/dense/kernel/adam_v (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_1/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_1/output/dense/kernel (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_1/output/dense/kernel/adam_m (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_1/output/dense/kernel/adam_v (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_10/attention/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/output/dense/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_10/attention/output/dense/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_10/attention/output/dense/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_10/attention/self/key/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/self/key/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/self/key/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/self/key/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_10/attention/self/key/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_10/attention/self/key/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_10/attention/self/query/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/self/query/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/self/query/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/self/query/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_10/attention/self/query/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_10/attention/self/query/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_10/attention/self/value/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/self/value/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/self/value/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/attention/self/value/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_10/attention/self/value/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_10/attention/self/value/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_10/intermediate/dense/bias (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_10/intermediate/dense/bias/adam_m (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_10/intermediate/dense/bias/adam_v (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_10/intermediate/dense/kernel (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_10/intermediate/dense/kernel/adam_m (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_10/intermediate/dense/kernel/adam_v (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_10/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_10/output/dense/kernel (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_10/output/dense/kernel/adam_m (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_10/output/dense/kernel/adam_v (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_11/attention/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/output/dense/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_11/attention/output/dense/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_11/attention/output/dense/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_11/attention/self/key/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/self/key/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/self/key/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/self/key/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_11/attention/self/key/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_11/attention/self/key/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_11/attention/self/query/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/self/query/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/self/query/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/self/query/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_11/attention/self/query/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_11/attention/self/query/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_11/attention/self/value/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/self/value/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/self/value/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/attention/self/value/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_11/attention/self/value/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_11/attention/self/value/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_11/intermediate/dense/bias (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_11/intermediate/dense/bias/adam_m (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_11/intermediate/dense/bias/adam_v (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_11/intermediate/dense/kernel (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_11/intermediate/dense/kernel/adam_m (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_11/intermediate/dense/kernel/adam_v (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_11/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_11/output/dense/kernel (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_11/output/dense/kernel/adam_m (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_11/output/dense/kernel/adam_v (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_2/attention/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/output/dense/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_2/attention/output/dense/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_2/attention/output/dense/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_2/attention/self/key/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/self/key/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/self/key/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/self/key/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_2/attention/self/key/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_2/attention/self/key/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_2/attention/self/query/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/self/query/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/self/query/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/self/query/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_2/attention/self/query/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_2/attention/self/query/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_2/attention/self/value/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/self/value/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/self/value/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/attention/self/value/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_2/attention/self/value/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_2/attention/self/value/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_2/intermediate/dense/bias (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_2/intermediate/dense/bias/adam_m (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_2/intermediate/dense/bias/adam_v (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_2/intermediate/dense/kernel (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_2/intermediate/dense/kernel/adam_m (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_2/intermediate/dense/kernel/adam_v (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_2/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_2/output/dense/kernel (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_2/output/dense/kernel/adam_m (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_2/output/dense/kernel/adam_v (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_3/attention/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/output/dense/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_3/attention/output/dense/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_3/attention/output/dense/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_3/attention/self/key/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/self/key/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/self/key/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/self/key/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_3/attention/self/key/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_3/attention/self/key/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_3/attention/self/query/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/self/query/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/self/query/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/self/query/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_3/attention/self/query/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_3/attention/self/query/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_3/attention/self/value/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/self/value/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/self/value/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/attention/self/value/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_3/attention/self/value/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_3/attention/self/value/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_3/intermediate/dense/bias (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_3/intermediate/dense/bias/adam_m (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_3/intermediate/dense/bias/adam_v (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_3/intermediate/dense/kernel (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_3/intermediate/dense/kernel/adam_m (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_3/intermediate/dense/kernel/adam_v (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_3/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_3/output/dense/kernel (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_3/output/dense/kernel/adam_m (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_3/output/dense/kernel/adam_v (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_4/attention/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/output/dense/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_4/attention/output/dense/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_4/attention/output/dense/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_4/attention/self/key/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/self/key/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/self/key/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/self/key/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_4/attention/self/key/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_4/attention/self/key/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_4/attention/self/query/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/self/query/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/self/query/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/self/query/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_4/attention/self/query/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_4/attention/self/query/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_4/attention/self/value/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/self/value/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/self/value/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/attention/self/value/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_4/attention/self/value/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_4/attention/self/value/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_4/intermediate/dense/bias (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_4/intermediate/dense/bias/adam_m (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_4/intermediate/dense/bias/adam_v (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_4/intermediate/dense/kernel (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_4/intermediate/dense/kernel/adam_m (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_4/intermediate/dense/kernel/adam_v (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_4/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_4/output/dense/kernel (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_4/output/dense/kernel/adam_m (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_4/output/dense/kernel/adam_v (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_5/attention/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/output/dense/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_5/attention/output/dense/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_5/attention/output/dense/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_5/attention/self/key/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/self/key/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/self/key/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/self/key/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_5/attention/self/key/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_5/attention/self/key/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_5/attention/self/query/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/self/query/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/self/query/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/self/query/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_5/attention/self/query/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_5/attention/self/query/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_5/attention/self/value/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/self/value/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/self/value/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/attention/self/value/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_5/attention/self/value/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_5/attention/self/value/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_5/intermediate/dense/bias (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_5/intermediate/dense/bias/adam_m (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_5/intermediate/dense/bias/adam_v (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_5/intermediate/dense/kernel (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_5/intermediate/dense/kernel/adam_m (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_5/intermediate/dense/kernel/adam_v (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_5/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_5/output/dense/kernel (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_5/output/dense/kernel/adam_m (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_5/output/dense/kernel/adam_v (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_6/attention/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/output/dense/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_6/attention/output/dense/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_6/attention/output/dense/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_6/attention/self/key/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/self/key/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/self/key/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/self/key/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_6/attention/self/key/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_6/attention/self/key/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_6/attention/self/query/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/self/query/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/self/query/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/self/query/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_6/attention/self/query/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_6/attention/self/query/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_6/attention/self/value/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/self/value/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/self/value/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/attention/self/value/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_6/attention/self/value/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_6/attention/self/value/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_6/intermediate/dense/bias (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_6/intermediate/dense/bias/adam_m (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_6/intermediate/dense/bias/adam_v (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_6/intermediate/dense/kernel (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_6/intermediate/dense/kernel/adam_m (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_6/intermediate/dense/kernel/adam_v (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_6/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_6/output/dense/kernel (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_6/output/dense/kernel/adam_m (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_6/output/dense/kernel/adam_v (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_7/attention/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/output/dense/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_7/attention/output/dense/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_7/attention/output/dense/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_7/attention/self/key/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/self/key/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/self/key/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/self/key/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_7/attention/self/key/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_7/attention/self/key/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_7/attention/self/query/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/self/query/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/self/query/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/self/query/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_7/attention/self/query/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_7/attention/self/query/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_7/attention/self/value/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/self/value/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/self/value/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/attention/self/value/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_7/attention/self/value/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_7/attention/self/value/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_7/intermediate/dense/bias (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_7/intermediate/dense/bias/adam_m (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_7/intermediate/dense/bias/adam_v (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_7/intermediate/dense/kernel (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_7/intermediate/dense/kernel/adam_m (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_7/intermediate/dense/kernel/adam_v (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_7/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_7/output/dense/kernel (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_7/output/dense/kernel/adam_m (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_7/output/dense/kernel/adam_v (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_8/attention/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/output/dense/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_8/attention/output/dense/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_8/attention/output/dense/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_8/attention/self/key/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/self/key/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/self/key/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/self/key/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_8/attention/self/key/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_8/attention/self/key/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_8/attention/self/query/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/self/query/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/self/query/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/self/query/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_8/attention/self/query/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_8/attention/self/query/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_8/attention/self/value/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/self/value/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/self/value/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/attention/self/value/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_8/attention/self/value/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_8/attention/self/value/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_8/intermediate/dense/bias (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_8/intermediate/dense/bias/adam_m (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_8/intermediate/dense/bias/adam_v (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_8/intermediate/dense/kernel (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_8/intermediate/dense/kernel/adam_m (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_8/intermediate/dense/kernel/adam_v (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_8/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_8/output/dense/kernel (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_8/output/dense/kernel/adam_m (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_8/output/dense/kernel/adam_v (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_9/attention/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/output/dense/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_9/attention/output/dense/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_9/attention/output/dense/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_9/attention/self/key/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/self/key/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/self/key/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/self/key/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_9/attention/self/key/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_9/attention/self/key/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_9/attention/self/query/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/self/query/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/self/query/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/self/query/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_9/attention/self/query/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_9/attention/self/query/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_9/attention/self/value/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/self/value/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/self/value/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/attention/self/value/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_9/attention/self/value/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_9/attention/self/value/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/bert/encoder/layer_9/intermediate/dense/bias (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_9/intermediate/dense/bias/adam_m (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_9/intermediate/dense/bias/adam_v (DT_FLOAT) [3072]\n",
      "module/bert/encoder/layer_9/intermediate/dense/kernel (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_9/intermediate/dense/kernel/adam_m (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_9/intermediate/dense/kernel/adam_v (DT_FLOAT) [768,3072]\n",
      "module/bert/encoder/layer_9/output/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/output/LayerNorm/beta/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/output/LayerNorm/beta/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/output/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/output/LayerNorm/gamma/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/output/LayerNorm/gamma/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/output/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/output/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/output/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/encoder/layer_9/output/dense/kernel (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_9/output/dense/kernel/adam_m (DT_FLOAT) [3072,768]\n",
      "module/bert/encoder/layer_9/output/dense/kernel/adam_v (DT_FLOAT) [3072,768]\n",
      "module/bert/pooler/dense/bias (DT_FLOAT) [768]\n",
      "module/bert/pooler/dense/bias/adam_m (DT_FLOAT) [768]\n",
      "module/bert/pooler/dense/bias/adam_v (DT_FLOAT) [768]\n",
      "module/bert/pooler/dense/kernel (DT_FLOAT) [768,768]\n",
      "module/bert/pooler/dense/kernel/adam_m (DT_FLOAT) [768,768]\n",
      "module/bert/pooler/dense/kernel/adam_v (DT_FLOAT) [768,768]\n",
      "module/cls/predictions/output_bias (DT_FLOAT) [30522]\n",
      "module/cls/predictions/transform/LayerNorm/beta (DT_FLOAT) [768]\n",
      "module/cls/predictions/transform/LayerNorm/gamma (DT_FLOAT) [768]\n",
      "module/cls/predictions/transform/dense/bias (DT_FLOAT) [768]\n",
      "module/cls/predictions/transform/dense/kernel (DT_FLOAT) [768,768]\n",
      "output_bias (DT_FLOAT) [20]\n",
      "output_bias/adam_m (DT_FLOAT) [20]\n",
      "output_bias/adam_v (DT_FLOAT) [20]\n",
      "output_weights (DT_FLOAT) [20,768]\n",
      "output_weights/adam_m (DT_FLOAT) [20,768]\n",
      "output_weights/adam_v (DT_FLOAT) [20,768]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "print_tensors_in_checkpoint_file(file_name=\"model.ckpt-0\", tensor_name='', all_tensors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     session.run(embed(corpus))\n",
    "\n",
    "\n",
    "# ### Save out embedding vectors\n",
    "print(\"save out embeddings\")\n",
    "np.save('embedded_clean_content'+os.path.basename(DATADIR)+'.npy', embedded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-similarity-3.6.4",
   "language": "python",
   "name": "content-similarity-3.6.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
