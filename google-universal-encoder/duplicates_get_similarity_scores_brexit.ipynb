{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T20:07:38.315414Z",
     "start_time": "2019-02-27T20:07:30.810889Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "r2A21W9EHF_p"
   },
   "outputs": [],
   "source": [
    "# import gensim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import pairwise_distances_chunked, pairwise_distances\n",
    "\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "\n",
    "from itertools import compress\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up and get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this notebook uses data derived from the [govuk-taxonomy-supervised-learning repo](https://github.com/alphagov/govuk-taxonomy-supervised-learning), we clone that. Given you are likely to perform this for a given date, we suggest you create a dir with the date of the format `dd-mm-yy` within the data folder of the aforementioned repo and point to the data folder therein as the DATADIR. \n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "/Users/adalovelace/Documents/govuk-taxonomy-supervised-learning/data/11-02-19\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T21:21:02.952850Z",
     "start_time": "2019-02-27T21:21:02.949999Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "muHD6w9CHa1Q",
    "outputId": "b268b330-b683-4fb9-a85e-8a0ef9817ac8"
   },
   "outputs": [],
   "source": [
    "DATADIR = os.getenv(\"DATADIR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import vectors with same index as labelled. This may require some modification to match your filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T20:42:02.898514Z",
     "start_time": "2019-02-27T20:42:02.219727Z"
    }
   },
   "outputs": [],
   "source": [
    "embedded_sentences = np.load('embedded_clean_content'+os.path.basename(DATADIR)+'.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Google Universal Sentence Encoder model takes a word, sentence or a paragraph as input and outputs a 512-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T20:42:06.497298Z",
     "start_time": "2019-02-27T20:42:06.492077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242861, 512)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows, cols\n",
    "embedded_sentences.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T20:13:55.078375Z",
     "start_time": "2019-02-27T20:13:34.728086Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GOZZ-trpszRZ"
   },
   "outputs": [],
   "source": [
    "content = pd.read_csv(\n",
    "    os.path.join(DATADIR, 'clean_content.csv'),\n",
    "    low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T20:28:37.271787Z",
     "start_time": "2019-02-27T20:28:37.267751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242861, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New variable, called level, to categorise taxon by level in the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique pieces of content are there on GOV.UK? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T20:51:20.624116Z",
     "start_time": "2019-02-27T20:51:20.552189Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7eEvWttRt7bm",
    "outputId": "fc6ca374-6408-4512-d147-77f86dabc487"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242861"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.content_id.nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get brexit subset of content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open('brexit_url_prefixes.txt', 'r')\n",
    "brexit_url_prefixes = text_file.read().split(',')\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1998"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brexit_url_prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### there are some prefixes which are urls to subsections of guides\n",
    "For example, this set of prefixes would only be represented once in content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/settled-status-eu-citizens-families/applying-for-settled-status',\n",
       " '/settled-status-eu-citizens-families',\n",
       " '/settled-status-eu-citizens-families/eligibility',\n",
       " '/settled-status-eu-citizens-families/what-youll-need-to-apply',\n",
       " '/settled-status-eu-citizens-families/what-settled-and-presettled-status-means',\n",
       " '/settled-status-eu-citizens-families/after-youve-applied',\n",
       " '/settled-status-eu-citizens-families/if-you-have-permanent-residence-or-indefinite-leave-to-remain',\n",
       " '/settled-status-eu-citizens-families/not-eu-eea-swiss-citizen',\n",
       " '/settled-status-eu-citizens-families/apply-settled-status-for-child',\n",
       " '/settled-status-eu-citizens-families/settled-status-less-than-5-years',\n",
       " '/settled-status-eu-citizens-families/print',\n",
       " '/settled-status-eu-citizens-families/when-to-apply',\n",
       " '/settled-status-eu-citizens-families#brexit',\n",
       " '/settled-status-eu-citizens-families/settled-status-if-youre-under-21',\n",
       " '/settled-status-eu-citizens-families/',\n",
       " '/settled-status-eu-citizens-families/what-youll-need-to-applyhttps://www.gov.uk/settled-status-eu-citizens-families/what-youll-need-to-apply',\n",
       " '/settled-status-eu-citizens-families/applying-for-settled-status-eu-citizens-families/eligibility',\n",
       " '/settled-status-eu-citizens-families/applying-for-settled-statushttps://www.gov.uk/settled-status-eu-citizens-families/applying-for-settled-status',\n",
       " '/settled-status-eu-citizens-families/applying-for-settled-statushttps:/www.gov.uk/settled-status-eu-citizens-families/applying-for-settled-status',\n",
       " '/settled-status-eu-citizens-families']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[prefix for prefix in brexit_url_prefixes if prefix.startswith('/settled-status-eu-citizens-families')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([prefix for prefix in brexit_url_prefixes if prefix.startswith('/settled-status-eu-citizens-families')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "the base_path in content will only have the first part of this brexit_url. So to find these in content \n",
    "\n",
    "- First find which prefixes have duplicate first parts\n",
    "e.g, \n",
    "'/settled-status-eu-citizens-families'\n",
    "- Then truncate them to root path \n",
    "- Add truncated root paths to lookup list (brexit_url_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitall(path):\n",
    "    \"\"\"split the prefix url into parts separated by /\n",
    "    returns a list containing each string part of path\"\"\"\n",
    "    allparts = []\n",
    "    while 1:\n",
    "        parts = os.path.split(path)\n",
    "        if parts[0] == path:  # sentinel for absolute paths\n",
    "            allparts.insert(0, parts[0])\n",
    "            break\n",
    "        elif parts[1] == path: # sentinel for relative paths\n",
    "            allparts.insert(0, parts[1])\n",
    "            break\n",
    "        else:\n",
    "            path = parts[0]\n",
    "            allparts.insert(0, parts[1])\n",
    "    return allparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture the first two parts into a list if there are more than 2 (there are a handful that are just 2) e.g., '/staying-uk-eu-citizen'\n",
    "first_2parts = []\n",
    "for path in brexit_url_prefixes:\n",
    "    parts = splitall(path)\n",
    "    if len(parts) >2:\n",
    "        first_2parts.append(parts[0]+parts[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1984"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_2parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find repeated items of the same truncated root path to identify the stem to search for in content\n",
    "seen = {}\n",
    "dupes = []\n",
    "\n",
    "for x in first_2parts:\n",
    "    if x not in seen:\n",
    "        seen[x] = 1\n",
    "    else:\n",
    "        if seen[x] == 1:\n",
    "            dupes.append(x)\n",
    "        seen[x] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/settled-status-eu-citizens-families',\n",
       " '/government',\n",
       " '/guidance',\n",
       " '/driving-abroad',\n",
       " '/world',\n",
       " '/eu-withdrawal-act-2018-statutory-instruments',\n",
       " '/print',\n",
       " '/prepare-eu-exit',\n",
       " '/visit-europe-brexit']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#these are the repeated stems. But some of these should be there. So pluck the guides out manually\n",
    "dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the stems to the lookup list so they can be matched to content\n",
    "brexit_url_prefixes = brexit_url_prefixes + ['/settled-status-eu-citizens-families',\n",
    " '/driving-abroad',\n",
    " '/eu-withdrawal-act-2018-statutory-instruments',\n",
    " '/prepare-eu-exit',\n",
    " '/visit-europe-brexit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T20:51:22.713741Z",
     "start_time": "2019-02-27T20:51:22.688235Z"
    }
   },
   "outputs": [],
   "source": [
    "#create a brexit flag as a pandas col\n",
    "content['brexit'] = np.where(content['base_path'].str.startswith(tuple(brexit_url_prefixes)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T20:51:23.934726Z",
     "start_time": "2019-02-27T20:51:23.925702Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    241386\n",
       "1      1475\n",
       "Name: brexit, dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there are fewer than len(brexit_url_preifx) because the list has repeated content items in it\n",
    "content.brexit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_sentences_brexit = embedded_sentences[content.brexit==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get similarity scores, brexit content by all GOV.UK content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_20_links(D_chunk, start):\n",
    "    \"\"\"return only the top 20 (including self) related link indices and distance metric values\n",
    "    according to distance metric\"\"\"\n",
    "    top_k_indices = np.argpartition(D_chunk, range(20))[:, :20]\n",
    "\n",
    "    return top_k_indices, D_chunk[:, top_k_indices]\n",
    "\n",
    "brexit_generator = pairwise_distances_chunked(\n",
    "    X=embedded_sentences_brexit,\n",
    "    Y=embedded_sentences,\n",
    "    reduce_func=get_top_20_links,\n",
    "    working_memory=0,\n",
    "    metric='cosine',\n",
    "    n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a0690c9392454184f26119e66cddc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ellieking/.pyenv/versions/3.6.4/envs/content-similarity-3.6.4/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "brexit_url = []\n",
    "close_content_urls = []\n",
    "urls = pd.DataFrame(columns=['brexit_url', 'close_content_urls', 'cosine_sims'])\n",
    "for i, (indices, values) in enumerate(tqdm_notebook(brexit_generator)):\n",
    "\n",
    "    brexit_url = content.iat[indices[0][0],\n",
    "                                     0]  #basepath is first column\n",
    "    close_content_urls = [content.iat[i, 0] for i in indices[0]]\n",
    "    #     cosine_sims = pd.Series(values)\n",
    "    i_urls = pd.DataFrame({\n",
    "        'brexit_url': brexit_url,\n",
    "        'close_content_urls': close_content_urls,\n",
    "        'cosine_sims': values.reshape(20)\n",
    "    })\n",
    "    urls = urls.append(i_urls, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls.to_csv(\"brexit_potential_dupes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "similarity_universal_sentence_embedding.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
