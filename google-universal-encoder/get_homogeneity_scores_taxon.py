# ------------------------------------------------------------------
# get_homogeneity_sores_taxon.py
#
# This script requires the following input files:
#     labelled.csv.gz - generated by the govuk-taxonomy-supervised-learning pipeline
#     embedded_clean_contentdata.npy - generated by the govuk-taxonomy-supervised-learning pipeline `make` command
#
# This script generates the following csv files
#
# taxon_homogeneity_scores.csv
#     Has the mean homogeneity score and other useful
#     data for each taxon, with each taxon as a row
#
# taxon_branch_homogeneity_scores.csv
#     Has the mean homogeneirt score and other useful
#     data for each branch (ie each top level taxon)
#     with each branch as a row
# ------------------------------------------------------------------

import numpy as np
import os
import pandas as pd
from sklearn.metrics import pairwise_distances_chunked, pairwise_distances
import sys

DATADIR = os.getenv("DATADIR")
if DATADIR is None:
    print("You must set a DATADIR environment variable, see the readme in alphagov/govuk-taxonomy-supervised-learning repo for more details")
    sys.exit()

# Read in data
embedded_clean_content = np.load(os.path.join(DATADIR, 'embedded_clean_content' + os.path.basename(DATADIR) + '.npy'))
labelled = pd.read_csv(
    os.path.join(DATADIR, 'labelled.csv.gz'),
    compression='gzip',
    low_memory=False)




# ------------------------------------------------------------------
# Generate taxon_homogeneity_scores.csv
# ------------------------------------------------------------------

# For each row, add a 'level' column with it's depth in the tree
labelled = labelled.assign(level=np.where(labelled.level5taxon.notnull(), 5, 0))
labelled.loc[labelled['level4taxon'].notnull() & labelled['level5taxon'].isnull(), 'level'] = 4
labelled.loc[labelled['level3taxon'].notnull() & labelled['level4taxon'].isnull(), 'level'] = 3
labelled.loc[labelled['level2taxon'].notnull() & labelled['level3taxon'].isnull(), 'level'] = 2
labelled.loc[labelled['level1taxon'].notnull() & labelled['level2taxon'].isnull(), 'level'] = 1

# Calculate the mean cosine score for each taxon
taxon_homogeneity = []
for taxon in labelled['taxon_id'].unique():
    taxon_embeddings = embedded_clean_content[labelled['taxon_id'] == taxon]
    taxon_size = taxon_embeddings.shape[0]
    mean_cosine_for_taxon = np.mean(
        pairwise_distances(taxon_embeddings, metric='cosine', n_jobs=-1))
    taxon_homogeneity.append([taxon, taxon_size, mean_cosine_for_taxon])

# Make a dataframe with all the data thus far
taxon_homogeneity_scores = pd.DataFrame(
    taxon_homogeneity, columns=['taxon_id', 'taxon_size',
                                'mean_cosine_score']).sort_values(
                                    'mean_cosine_score', ascending=False)

# Add extra columns
taxon_id_to_base_path = dict(zip(labelled['taxon_id'], labelled['taxon_base_path']))
taxon_homogeneity_scores['taxon_base_path'] = taxon_homogeneity_scores['taxon_id'].map(taxon_id_to_base_path)

taxon_id_to_level = dict(zip(labelled['taxon_id'], labelled['level']))
taxon_homogeneity_scores['taxon_level'] = taxon_homogeneity_scores['taxon_id'].map(taxon_id_to_level)

taxon_id_to_level1 = dict(zip(labelled['taxon_id'], labelled['level1taxon']))
taxon_homogeneity_scores['level1taxon'] = taxon_homogeneity_scores['taxon_id'].map(taxon_id_to_level1)

taxon_homogeneity_scores['fewer_than_or_equal_5items'] = np.where(taxon_homogeneity_scores['taxon_size'] <= 5, 1, 0)
taxon_homogeneity_scores['more_than_0_5_diversity'] = np.where(taxon_homogeneity_scores['mean_cosine_score'] > 0.5, 1, 0)

# Put it into a csv
taxon_homogeneity_scores.to_csv("taxon_homogeneity_scores.csv")




# ------------------------------------------------------------------
# Generate taxon_branch_homogeneity_scores.csv
# ------------------------------------------------------------------

# Calculate size and mean cosine scores for each branch
branch_homogeneity = []
for branch in labelled.level1taxon.unique():
    total_cosine_for_branch = np.zeros(1)
    denominator_for_branch = np.zeros(1)
    branch_embeddings = embedded_clean_content[labelled['level1taxon'] == branch]
    branch_size = branch_embeddings.shape[0]
    for chunk in pairwise_distances_chunked(branch_embeddings, metric='cosine', n_jobs=-1):
        total_cosine_for_branch += np.sum(chunk)
        denominator_for_branch += np.prod(chunk.shape)
    mean_cosine_for_branch = (total_cosine_for_branch / denominator_for_branch).item()
    branch_homogeneity.append([branch, branch_size, mean_cosine_for_branch])

# Put mean cosine scores for each branch into a sorted dataframe
branch_homogeneity_scores = pd.DataFrame(branch_homogeneity, columns=['branch', 'branch_size', 'mean_cosine_score']) \
    .sort_values('mean_cosine_score', ascending=False)

# Add weighted score
branch_homogeneity_scores['min_max_branch_size'] = (
     branch_homogeneity_scores['branch_size'] -
     branch_homogeneity_scores['branch_size'].min()) / (
     branch_homogeneity_scores['branch_size'].max() -
     branch_homogeneity_scores['branch_size'].min())
branch_homogeneity_scores['weighted_score'] = branch_homogeneity_scores['mean_cosine_score'] / branch_homogeneity_scores['min_max_branch_size']

# Add branch to taxon ratio
mean_per_level_1_taxon = taxon_homogeneity_scores[['taxon_size', 'mean_cosine_score', 'level1taxon', 'fewer_than_or_equal_5items']] \
    .groupby('level1taxon').mean()
mean_per_level_1_taxon['weighted_score_taxon'] = mean_per_level_1_taxon['mean_cosine_score'] / mean_per_level_1_taxon['taxon_size'] * 1000
branch_by_mean_taxon = pd.merge(mean_per_level_1_taxon, branch_homogeneity_scores, left_on='level1taxon', right_on='branch')
branch_homogeneity_scores['branch_to_taxon_ratio'] = branch_by_mean_taxon['mean_cosine_score_y']/branch_by_mean_taxon['mean_cosine_score_x']

branch_homogeneity_scores.to_csv("taxon_branch_homogeneity_scores.csv")